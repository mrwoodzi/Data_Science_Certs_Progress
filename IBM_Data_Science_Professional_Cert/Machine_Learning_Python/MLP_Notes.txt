What is Machine Learning:
    Inductive Inference
    Machine Learning   
        -used for estimating relationships
    An AI domain where we extract patterns from the data and analyze 
        the data and make intelligent predictions on the new data
        according to the pattern your machine has learnt
    How machines extract patterns?
    How we analyze the data?
    How machines make predictions?
    Tom Mitchell
        "A computer program is said to learn from experience E with respect to some task T 
            and some performance measure P, if its performance on T, as measured by P, 
            improves with experience E."
    Experience -> Task -> Performance -> Improve -> Performance -> Improve 

    Experience  
        -can be past data
    Task    
        -the prediction you are trying to make 
    Performance 
        -how well your model is performing on new and unseen data
    Improve 
        -feedback

Linear Regression:
    -extracting patterns

    Example:
        Prepare a marketing plan for a Car Company
            -manufacturer of the car
            -model of the car 
            -engine size of car 
            -horsepower of the car 

        Making a market plan    
            -recommend which info of the car to highlight
                -to make the most of Stakeholders
            Is there a relationshiop between info of the car and sales?
            How strong is the relationship between info of the car and sales?
            Which info contributes to sales?
            How accurately can we estimate the effect of each info on sales?
            How accurately can we predict the sales?
            Is the relationship linear?


Module 1:
    Introduction and Regression
        
    Simple Linear Regression 
    Multiple Linear Regression 
    Regression Trees 



Module 2:
    Classification 
        Logistic Regression
        KNN 
        SVM 
        Multiclass Prediction 
        Decision Trees
            -uses historical data 
 


Module 3:
    Clustering 
        K-means 


Module 4:
    Final Project


Data Science Methodology Review:
Business Understanding
Analytic Approach
Data Requirements
Data Collection
Data Understanding
Data Preparation 
Modeling
Evaluation 
Deployment 
Feedback
***Process is Iterative***


Machine Learning is the subfield of computer science that gives "computers the ability to learn without 
        being explicitly programmed"

        Define "without being explicitly programmed."


Major Machine Learning Techniques:
Regression:
    -predicting continuous values
Classification:
    -predicting class or category of a cases 
Clustering:
    -finding the structure of data; summarization
    - ex., grouping of similar cases in a dataset, for example to find similar patients, or for customer segmentation in a bank 
Associations:
    -associating frequent co-occurring items/events
Anomaly detection:
    -discovering abnormal and unusual cases 
Sequence mining:
    -predicting next events; click-stream (Markov Model, HMM)
Dimension Reduction:
    -reducing the size of data (PCA) 
Recommendation systems:
    -recommending items


Python Libraries for Deep Learning:
    Numpy
        images
        functions
        data types
    SciPy
        -signal preprocessing
        -optomization 
        -statistics
    Matplotlib
        -plotting package that provides 2d plotting and 3d plotting
    Pandas 
        -high level library 
        -data structures, analysis, structures 
        -operations for maipulating numerical tables and timeseries
    SciKit Learn 
        -machine learning library
        -collection of algorithms 
        -Popular among data scientist
            -free 
            -has most of the typical classification, regression and clustering algorithms
            -design to work with numpy and scipy 
            -good documentation
            -it's easy 
        -pre-processing of data 
        -feature selection
        -freature extraction
        -train test splitting
        -defining the algorithm
        -fitting models 
        -tuning parameters
        -prediction 
        -Evaluation
        -exporting model 


*****************Supervised Algorithms vs. Unsupervised Algorithms**************
column_names = attributes
column_data = features
row = observation

2 types of Supervised Learning:
Classification  
    -process of predicting a discreate class label or category
    -K-nearest neighbor 
    -decision Trees 
    -linear discriminant analysis 
    -naive bayes
    -linear discriminant analysis
    -k-nearest neighbor 
    -logistic regression 
    -neural networks 
    -support vector machines 
Regression
    -process of predicting a continuous value 

Unsupervised Learning Techniques:
    -draws conclusions on unlabeled data 
    -clustering
        -used for grouping data points or objects that are somehow similar 
        -also used for discovering structure, summarization and anomaly detection 
    -dimension Reduction
        -reduces redundant features to make the classification easier
    -density estimation 
        -explores the data to find some particular structure 
    -market basket analysis 
        -theory that if you buy a certain group of items, you're more likely to buy another group of items

There are different model evaluation metrics, lets use MSE here to calculate the accuracy of our model based on the test set: 

* Mean Absolute Error: It is the mean of the absolute value of the errors. This is the easiest of the metrics to understand 
            since it’s just average error.

* Mean Squared Error (MSE): Mean Squared Error (MSE) is the mean of the squared error. It’s more popular than Mean 
            Absolute Error because the focus is geared more towards large errors. This is due to the squared term 
            exponentially increasing larger errors in comparison to smaller ones.

* Root Mean Squared Error (RMSE). 

* R-squared is not an error, but rather a popular metric to measure the performance of your regression model. It represents 
            how close the data points are to the fitted regression line. The higher the R-squared value, the better the model 
            fits your data. The best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse).


As mentioned before, __Coefficient__ and __Intercept__  are the parameters of the fitted line. 
Given that it is a multiple linear regression model with 3 parameters and that the parameters are 
the intercept and coefficients of the hyperplane, sklearn can estimate them from our data. 
Scikit-learn uses plain Ordinary Least Squares method to solve this problem.

#### Ordinary Least Squares (OLS)
OLS is a method for estimating the unknown parameters in a linear regression model. OLS chooses 
the parameters of a linear function of a set of explanatory variables by minimizing the sum of 
the squares of the differences between the target dependent variable and those predicted by the 
linear function. In other words, it tries to minimizes the sum of squared errors (SSE) or mean 
squared error (MSE) between the target variable (y) and our predicted output ($\hat{y}$) over 
all samples in the dataset.

OLS can find the best parameters using the following methods:
* Solving the model parameters analytically using closed-form equations
* Using an optimization algorithm (Gradient Descent, Stochastic Gradient Descent, Newton’s Method, etc.)
    

steps of Linear Regression:
# 1.
# Create a LinearRegression model it is instantiated as an object
regr = linear_model.LinearRegression()

# 2.
# Prepare the input features (x) and the target variable (y)
# In this example, x includes 'ENGINESIZE', 'CYLINDERS', and 'FUELCONSUMPTION_COMB'
x = np.asanyarray(train[['ENGINESIZE','CYLINDERS','FUELCONSUMPTION_CITY','FUELCONSUMPTION_HWY']])
y = np.asanyarray(train[['CO2EMISSIONS']])

# 3.
# Fit the LinearRegression model to the data
regr.fit (x, y)

# 4.
# Print the coefficients of the linear model
print ('Coefficients: ', regr.coef_)

# 5.
# y_hat are the predictions
y_hat = regr.predict(test[['ENGINESIZE','CYLINDERS','FUELCONSUMPTION_CITY','FUELCONSUMPTION_HWY']])

# 6.
# Prepare the input features (x) for the test data.
x = np.asanyarray(test[['ENGINESIZE','CYLINDERS','FUELCONSUMPTION_CITY','FUELCONSUMPTION_HWY']])

# 7.
# Prepare the target variable (y) for the test data, which is the actual CO2 emissions.
y = np.asanyarray(test[['CO2EMISSIONS']])

# 8.
# Calculate the Mean Squared Error (MSE) between the predicted values (y_hat) and the actual values (y).
mse = np.mean((y_hat - y) ** 2)
print("Mean Squared Error (MSE): %.2f" % mse)

# 9.
# The Explained Variance Score (Variance score) measures how well the model explains the variance in the data.
# A score of 1 indicates a perfect prediction, while lower scores indicate less accurate predictions.
variance_score = regr.score(x, y)
print('Variance score: %.2f' % variance_score)

Classification:
    -Supervised Learning
    -Target Attribute is a Categorical Variable
    -good example of classification is the loan default prediction - column labeled "default"
        -LOAN DEFAULT PREDICTOR
            -use existing loan data to predict a 0 or 1 as 'defaulter' or 'not defaulter'
    -Can be binary or multi-class classification 


    Classification Accuracy: 
        -pass the test set to our model and we get y^(y-hat) predicted labels 
        -compare actual (y) to (y^)

        Evaluation Metrics:
            Jaccard Index
                -simplest
                -also known as Jaccard similarity coefficient
                -defined Jaccard as the size of the intersection divided by the size of the union of two label sets
                    -2 circles that are partially overlapping
                -If the entire set of predicted labels for a sample strictly matches 
                    with the true set of labels, then the subset accuracy is 1.0
            F1-score
            Log Loss


K-nearest neighbor:

    x = independent variable(s), rows and columns together
    y = dependent variable, a column
        -you can make custom categories for individual rows to be categorized as in 
            the dependent variable column, the one you are classifying
    -method for classifying cases based on their similarity to other cases 
    -cases that are near each other are said to be "neighbors"
    -based on similar cases with the same lass clabels are near each other
    -kNN can also be used for regression 
        -predicting home value by it's features 
        



kNN Steps:
    1. Pick a value for k
        -k in kNN = number of nearest neighbors to examine 
        -low value of k causes a highly complex model, possible overfit of the model 
        -high value becomes overly generalized 
        -reserve part of data for testing the accuracy of the model 
    2. calculate the distance of unknown case from all cases 
            -calculating the similarity/distance in a 1-dimensional space 
                -this would be 2 customers difference in age 
                    this would be Minkowsk/Euclidean distance 
                -2 dimension/2 features 
                    -age and income
                -multi dimensional vectors 
    3. select the K-observations in the training data that are "nearest 
        to the unknown data point 
    4. Predict the response of the unknown data point using the most popular response value from the K-nearest neighbors 


Decision Trees:



Regression Trees:


