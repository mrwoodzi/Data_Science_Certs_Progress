What is Machine Learning:
    Inductive Inference
    Machine Learning   
        -used for estimating relationships
    An AI domain where we extract patterns from the data and analyze 
        the data and make intelligent predictions on the new data
        according to the pattern your machine has learnt
    How machines extract patterns?
    How we analyze the data?
    How machines make predictions?
    Tom Mitchell
        "A computer program is said to learn from experience E with respect to some task T 
            and some performance measure P, if its performance on T, as measured by P, 
            improves with experience E."
    Experience -> Task -> Performance -> Improve -> Performance -> Improve 

    Experience  
        -can be past data
    Task    
        -the prediction you are trying to make 
    Performance 
        -how well your model is performing on new and unseen data
    Improve 
        -feedback

Linear Regression:
    -extracting patterns

    Example:
        Prepare a marketing plan for a Car Company
            -manufacturer of the car
            -model of the car 
            -engine size of car 
            -horsepower of the car 

        Making a market plan    
            -recommend which info of the car to highlight
                -to make the most of Stakeholders
            Is there a relationshiop between info of the car and sales?
            How strong is the relationship between info of the car and sales?
            Which info contributes to sales?
            How accurately can we estimate the effect of each info on sales?
            How accurately can we predict the sales?
            Is the relationship linear?


Module 1:
    Introduction and Regression
        
    Simple Linear Regression 
    Multiple Linear Regression 
    Regression Trees 



Module 2:
    Classification 
        Logistic Regression
        KNN 
        SVM 
        Multiclass Prediction 
        Decision Trees
            -uses historical data 
 


Module 3:
    Clustering 
        K-means 


Module 4:
    Final Project


Data Science Methodology Review:
Business Understanding
Analytic Approach
Data Requirements
Data Collection
Data Understanding
Data Preparation 
Modeling
Evaluation 
Deployment 
Feedback
***Process is Iterative***


Machine Learning is the subfield of computer science that gives "computers the ability to learn without 
        being explicitly programmed"

        Define "without being explicitly programmed."


Major Machine Learning Techniques:
Regression:
    -predicting continuous values
Classification:
    -predicting class or category of a cases 
Clustering:
    -finding the structure of data; summarization
    - ex., grouping of similar cases in a dataset, for example to find similar patients, or for customer segmentation in a bank 
Associations:
    -associating frequent co-occurring items/events
Anomaly detection:
    -discovering abnormal and unusual cases 
Sequence mining:
    -predicting next events; click-stream (Markov Model, HMM)
Dimension Reduction:
    -reducing the size of data (PCA) 
Recommendation systems:
    -recommending items




Confusion Matrix:
     confusion matrix is that it shows the model’s ability to correctly predict or separate the classes





Python Libraries for Deep Learning:
    Numpy
        images
        functions
        data types
    SciPy
        -signal preprocessing
        -optomization 
        -statistics
    Matplotlib
        -plotting package that provides 2d plotting and 3d plotting
    Pandas 
        -high level library 
        -data structures, analysis, structures 
        -operations for maipulating numerical tables and timeseries
    SciKit Learn 
        -machine learning library
        -collection of algorithms 
        -Popular among data scientist
            -free 
            -has most of the typical classification, regression and clustering algorithms
            -design to work with numpy and scipy 
            -good documentation
            -it's easy 
        -pre-processing of data 
        -feature selection
        -freature extraction
        -train test splitting
        -defining the algorithm
        -fitting models 
        -tuning parameters
        -prediction 
        -Evaluation
        -exporting model 


*****************Supervised Algorithms vs. Unsupervised Algorithms**************
column_names = attributes
column_data = features
row = observation

2 types of Supervised Learning:
Classification  
    -process of predicting a discreate class label or category
    -K-nearest neighbor 
    -decision Trees 
    -linear discriminant analysis 
    -naive bayes
    -linear discriminant analysis
    -k-nearest neighbor 
    -logistic regression 
    -neural networks 
    -support vector machines 
Regression
    -process of predicting a continuous value 

Unsupervised Learning Techniques:
    -draws conclusions on unlabeled data 
    -clustering
        -used for grouping data points or objects that are somehow similar 
        -also used for discovering structure, summarization and anomaly detection 
    -dimension Reduction
        -reduces redundant features to make the classification easier
    -density estimation 
        -explores the data to find some particular structure 
    -market basket analysis 
        -theory that if you buy a certain group of items, you're more likely to buy another group of items

There are different model evaluation metrics, lets use MSE here to calculate the accuracy of our model based on the test set: 

* Mean Absolute Error: It is the mean of the absolute value of the errors. This is the easiest of the metrics to understand 
            since it’s just average error.

* Mean Squared Error (MSE): Mean Squared Error (MSE) is the mean of the squared error. It’s more popular than Mean 
            Absolute Error because the focus is geared more towards large errors. This is due to the squared term 
            exponentially increasing larger errors in comparison to smaller ones.

* Root Mean Squared Error (RMSE). 

* R-squared is not an error, but rather a popular metric to measure the performance of your regression model. It represents 
            how close the data points are to the fitted regression line. The higher the R-squared value, the better the model 
            fits your data. The best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse).


As mentioned before, __Coefficient__ and __Intercept__  are the parameters of the fitted line. 
Given that it is a multiple linear regression model with 3 parameters and that the parameters are 
the intercept and coefficients of the hyperplane, sklearn can estimate them from our data. 
Scikit-learn uses plain Ordinary Least Squares method to solve this problem.

#### Ordinary Least Squares (OLS)
OLS is a method for estimating the unknown parameters in a linear regression model. OLS chooses 
the parameters of a linear function of a set of explanatory variables by minimizing the sum of 
the squares of the differences between the target dependent variable and those predicted by the 
linear function. In other words, it tries to minimizes the sum of squared errors (SSE) or mean 
squared error (MSE) between the target variable (y) and our predicted output ($\hat{y}$) over 
all samples in the dataset.

OLS can find the best parameters using the following methods:
* Solving the model parameters analytically using closed-form equations
* Using an optimization algorithm (Gradient Descent, Stochastic Gradient Descent, Newton’s Method, etc.)
    

steps of Linear Regression:
# 1.
# Create a LinearRegression model it is instantiated as an object
regr = linear_model.LinearRegression()

# 2.
# Prepare the input features (x) and the target variable (y)
# In this example, x includes 'ENGINESIZE', 'CYLINDERS', and 'FUELCONSUMPTION_COMB'
x = np.asanyarray(train[['ENGINESIZE','CYLINDERS','FUELCONSUMPTION_CITY','FUELCONSUMPTION_HWY']])
y = np.asanyarray(train[['CO2EMISSIONS']])

# 3.
# Fit the LinearRegression model to the data
regr.fit (x, y)

# 4.
# Print the coefficients of the linear model
print ('Coefficients: ', regr.coef_)

# 5.
# y_hat are the predictions
y_hat = regr.predict(test[['ENGINESIZE','CYLINDERS','FUELCONSUMPTION_CITY','FUELCONSUMPTION_HWY']])

# 6.
# Prepare the input features (x) for the test data.
x = np.asanyarray(test[['ENGINESIZE','CYLINDERS','FUELCONSUMPTION_CITY','FUELCONSUMPTION_HWY']])

# 7.
# Prepare the target variable (y) for the test data, which is the actual CO2 emissions.
y = np.asanyarray(test[['CO2EMISSIONS']])

# 8.
# Calculate the Mean Squared Error (MSE) between the predicted values (y_hat) and the actual values (y).
mse = np.mean((y_hat - y) ** 2)
print("Mean Squared Error (MSE): %.2f" % mse)

# 9.
# The Explained Variance Score (Variance score) measures how well the model explains the variance in the data.
# A score of 1 indicates a perfect prediction, while lower scores indicate less accurate predictions.
variance_score = regr.score(x, y)
print('Variance score: %.2f' % variance_score)

Classification:
    -Supervised Learning
    -Target Attribute is a Categorical Variable
    -good example of classification is the loan default prediction - column labeled "default"
        -LOAN DEFAULT PREDICTOR
            -use existing loan data to predict a 0 or 1 as 'defaulter' or 'not defaulter'
    -Can be binary or multi-class classification 


    Classification Accuracy: 
        -pass the test set to our model and we get y^(y-hat) predicted labels 
        -compare actual (y) to (y^)
        -precision = True Positive / (True Positive + False Positive)
            -Precision is a measure of the accuracy, provided that a class label has been predicted
        -Recall = True Positive / (True Positive + False Negative)
            -Recall is the True positive rate
        -Accuracy classification score computes subset accuracy: the set of labels predicted for a sample must exactly 
            match the corresponding set of labels in y_true.
        -multilabel classification, the function returns the subset accuracy. If the entire set of predicted labels 
            for a sample strictly matches with the true set of labels, then the subset accuracy is 1.0; 
            otherwise it is 0.0
        Evaluation Metrics:
            Jaccard Index
                -simplest
                -also known as Jaccard similarity coefficient
                -defined Jaccard as the size of the intersection divided by the size of the union of two label sets
                    -2 circles that are partially overlapping
                -If the entire set of predicted labels for a sample strictly matches 
                    with the true set of labels, then the subset accuracy is 1.0
            F1-score
                -harmonic average of the precision and recall
                F1 score reaches its best value at 1 (which represents perfect 
                    precision and recall) and its worst at 0
                            -precision = True Positive / (True Positive + False Positive)
                            -Precision is a measure of the accuracy, provided that a class label has been predicted
                            -Recall = True Positive / (True Positive + False Negative)
                            -Recall is the True positive rate
                        -precision = tp/(tp + fp)
                        -recall = tp/(tp + fn)
                        -F1-score = 2x(prc x rec)/(prc + rec)
            Log Loss


K-nearest neighbor:

    x = independent variable(s), rows and columns together
    y = dependent variable, a column
        -you can make custom categories for individual rows to be categorized as in 
            the dependent variable column, the one you are classifying
    -method for classifying cases based on their similarity to other cases 
    -cases that are near each other are said to be "neighbors"
    -based on similar cases with the same lass clabels are near each other
    -kNN can also be used for regression 
        -predicting home value by it's features 
    -build a classifier, to predict the class of unknown cases. 
        We will use a specific type of classification called K nearest neighbour
        



kNN Steps:
    1. Convert pandas df to numpy array with scikit Learn
        X = df[['column1', 'column2', 'column3', 'column4']]
    2. Make y variable:
        y = df['custcat also known as dependent variable']
    3. Normalize Data:
        X - preprocessing.StandardScaler().fit(x).transform(X.astype(float))
    4. Train, Test, Split:
        from sklearn.model_selection import train_test_split
        X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=4)
        print ('Train set:', X_train.shape,  y_train.shape)
        print ('Test set:', X_test.shape,  y_test.shape)
    5. kNN  
        from sklearn.neighbors import KNeighborsClassifier
        k = 4
        #Train Model and Predict  
        neigh = KNeighborsClassifier(n_neighbors = k).fit(X_train,y_train)
        neigh
    6. Predicting:
        yhat = neigh.predict(X_test)
    7. Accuract Evaluation:
        from sklearn import metrics
        print("Train set Accuracy: ", metrics.accuracy_score(y_train, neigh.predict(X_train)))
        print("Test set Accuracy: ", metrics.accuracy_score(y_test, yhat))

kNN Calculating accuract for different values of k:
    1. Calculate Accuracy for different values of k:
            Ks = 10
            mean_acc = np.zeros((Ks-1))
            std_acc = np.zeros((Ks-1))

            for n in range(1,Ks):
                
                #Train Model and Predict  
                neigh = KNeighborsClassifier(n_neighbors = n).fit(X_train,y_train)
                yhat=neigh.predict(X_test)
                mean_acc[n-1] = metrics.accuracy_score(y_test, yhat)

                
                std_acc[n-1]=np.std(yhat==y_test)/np.sqrt(yhat.shape[0])

            mean_acc

    2. Plot the Model Accuracy for Different number of neighbors 
            plt.plot(range(1,Ks),mean_acc,'g')
            plt.fill_between(range(1,Ks),mean_acc - 1 * std_acc,mean_acc + 1 * std_acc, alpha=0.10)
            plt.fill_between(range(1,Ks),mean_acc - 3 * std_acc,mean_acc + 3 * std_acc, alpha=0.10,color="green")
            plt.legend(('Accuracy ', '+/- 1xstd','+/- 3xstd'))
            plt.ylabel('Accuracy ')
            plt.xlabel('Number of Neighbors (K)')
            plt.tight_layout()
            plt.show()       

    3. Which k Has the Best Accuracy:
            print( "The best accuracy was with", mean_acc.max(), "with k=", mean_acc.argmax()+1) 




    1. Pick a value for k
        -k in kNN = number of nearest neighbors to examine 
        -low value of k causes a highly complex model, possible overfit of the model 
        -high value becomes overly generalized 
        -reserve part of data for testing the accuracy of the model 
    2. calculate the distance of unknown case from all cases 
            -calculating the similarity/distance in a 1-dimensional space 
                -this would be 2 customers difference in age 
                    this would be Minkowsk/Euclidean distance 
                -2 dimension/2 features 
                    -age and income
                -multi dimensional vectors 
    3. select the K-observations in the training data that are "nearest 
        to the unknown data point 
    4. Predict the response of the unknown data point using the most popular response value from the K-nearest neighbors 


Decision Trees:
    -Building a model from data to predict a class 
    -Sklearn Decision Trees do not handle categorical variables. We can still convert these features to numerical 
        values using **pandas.get_dummies() to convert the categorical variable into dummy/indicator variables
    Steps:
    1. Declare X remembering to remove the target set(y):
        # X is the Feature Matrix(data of df)
        # y is the Response Vector(target)
        X = df[['Age', 'Sex', 'BP', 'Cholesterol', 'Na_to_K']].values
    2. 
            from sklearn import preprocessing
            le_sex = preprocessing.LabelEncoder()
            le_sex.fit(['F','M'])
            X[:,1] = le_sex.transform(X[:,1]) 

            le_BP = preprocessing.LabelEncoder()
            le_BP.fit([ 'LOW', 'NORMAL', 'HIGH'])
            X[:,2] = le_BP.transform(X[:,2])

            le_Chol = preprocessing.LabelEncoder()
            le_Chol.fit([ 'NORMAL', 'HIGH'])
            X[:,3] = le_Chol.transform(X[:,3]) 
    3. Fill the target variable:
            y = df['Drug']
    4. Setting up Decision Tree:
            from sklearn.model_selection import train_test_split
    5. Train, Test, Split:
            X_trainset, X_testset, y_trainset, y_testset = train_test_split(X, y, test_size=0.3, random_state=3)
    6. Ensure Dimensions Match Train:
            print(f'Shape of X_trainset: {X_trainset.shape}, Size of y_trainset: {y_trainset.shape}')
    7. Ensure Dimensions Match Test:
            print(f'Shape of X_trainset: {X_trainset.shape}, Size of y_trainset: {y_trainset.shape}')
    8. Create Instance of Decision Tree Classifier:
            # criterion="entropy" can see the information gain on each node
            drugTree = DecisionTreeClassifier(criterion="entropy", max_depth = 4)
            drugTree # it shows the default parameters
    9. Fit data with training feature matrix X_trainset and train response vector y_trainset    
            drugTree.fit(X_trainset, y_trainset)
    10. Making Predictions:
            predTree = drugTree.predict(X_testset)
    10.5 If you want to visually compare the predictions to actual values 
            print (predTree [0:5])
            print (y_testset [0:5])     
    11. Check for Accuracy:
            from sklearn import metrics
            import matplotlib.pyplot as plt
            print("DecisionTrees's Accuracy: ", metrics.accuracy_score(y_testset, predTree))            
    12. Visualize:
            tree.plot_tree(drugTree)
            plt.show()


Regression Trees:

        IBM Library called Snap ML 

        Regression Trees are implemented using `DecisionTreeRegressor` from `sklearn.tree`
        The important parameters of `DecisionTreeRegressor` are
        `criterion`: {"mse", "friedman_mse", "mae", "poisson"} - The function used to measure error
        `max_depth` - The max depth the tree can be
        `min_samples_split` - The minimum number of samples required to split a node
        `min_samples_leaf` - The minimum number of samples that a leaf can contain
        `max_features`: {"auto", "sqrt", "log2"} - The number of feature we examine looking 
            for the best one, used to speed up training


    Steps:
    1. Import Necessary Libraries:
            # Pandas will allow us to create a dataframe of the data so it can be used and manipulated
            import pandas as pd
            # Regression Tree Algorithm
            from sklearn.tree import DecisionTreeRegressor
            # Split our data into a training and testing data
            from sklearn.model_selection import 
    2. Split df into features:
            X = df.drop(columns=["MEDV"])
            Y = df["MEDV"]
    3. Split into training and testing:
            X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=.2, random_state=1)
    4. Create Regression Tree:
            regression_tree = DecisionTreeRegressor(criterion = "squared_error")
    5. Train the Model:
            regression_tree.fit(X_train, Y_train)
    6. Evaluation using Score method of DecisionTreeRegressor:
            # this number is the $R^2$ value which indicates the coefficient of determination
            regression_tree.score(X_test, Y_test)
    7. Finding Average Error in our testing set, which is the average Error 
        in median home value prediction:
            prediction = regression_tree.predict(X_test)
            print("$",(prediction - Y_test).abs().mean()*1000)
    7a. Example of absolute error whole:
    # Train regression tree using absolute_error as the criterion
            regression_tree = DecisionTreeRegressor(criterion="absolute_error")
            regression_tree.fit(X_train, Y_train)
            regression_tree.score(X_test, Y_test)
            print(regression_tree.score(X_test, Y_test))
            prediction = regression_tree.predict(X_test)
            print("$",(prediction - Y_test).abs().mean()*1000)


Snap ML:
    refer to Regression_Trees_SnapML.ipynb for the indepth notes
    
 advantages of using Snap ML: acceleration of training of classical machine learning models, 
    such as linear and tree-based models.

    

Logistics Regression:
    -Classification
        -Probability of a Class 
        -"S" shaped 
    -Used for predicting the class of each case/ of an observed data point
        -guides on what would be the most probable class of a data point
    -Variant of Linear Regression
    -used when the observed dependent variable, y, is categorical 
        -produces a formula that predicts the probability of the class label as a function  
            of the independent variables
    -fits a special s-shaped curve by taking the linear regression function and transforming the    
        numeric estrimate into a probability with the following function, which is called the 
        sigmoid function (greek sign that looks circular)
    Examples:
        Helps you to predict what behavior will help you to retain customers
            -you can analyze all relevant customer data and develop focused customer 
                retention programs

    LogisticRegression function in sklearn:
        -implements logistic regression and can use different numerical optimizers 
            to find parameters, including'newton-cg', 'lbfgs', 'liblinear', 'sag',
            'saga' solvers. ***look up pros and cons of these dependent on project.
        -supports regularization which is technique used to solve the overfitting problem 
            of machine learning models.
                            Overfitting review - performs well on the training 
                            data but fails to generalize effectively to new, unseen data

        Evaluation of LogisticRegression, jaccard index and confusion matrix: 
            jaccard index   
                -accuracy evaluation
                    -size of the intersection divided by the size of the union of the 
                        two label sets.
                        -if the entire set of predicted labels for a sample strictly matches    
                            with the true set of labels then the subset accuracty is 1.0;
                            otherwise 0.0.
            confusion matrix        
                -looking at accuracy of the classifier

        Classification Report
            -classification_report
                -column 'precision'
                    -measure of the accuracy provided that a class label has been 
                        predicted. It is defiend by: precision = TP/(TP + FP)
                -column 'recall 
                    -true positive rate. deficned as: recall = TP/(TP + FN)
                -column 'f1-score'
                    -based on 'precision' and 'recall'
                    -harmonic average of the precision and recall, where f1-score reaches 
                            it's best value at 1(perfect precision and recall) and worst at 0. 
                        -good way to show that a classifier has a good value for both recall 
                            and precision 
                        


    Logistic Regresssion Steps:
        1. Extract, Transform, Load 
            -extract features from df
            -transform target data to integer, as it is required by sklearn algorithm
        2. Define X and y: 
            X = np.asarray(df[['column1', 'column2', 'column3', 'etc...']])
            y = np.asarray(df['target_data_column_converted_to_int'])
        3. Normalize:
            X = preprocessing.StandardScaler().fit(X).transform(x)
            x[0:5]
        4. Train/Test dataset:
            from sklearn.model_selection import train_test_split
            X_train, X_test, y_train, y_test, = train_test_split(X, y, test_size=0.2, random_state=4)
            print('Train set:', X_train.shape, y_train.shape)
            print('Test set:', X_test.shape, y_test.shape)
        5. Build Model:
                        "The optimization algorithm used to find the parameters 
                        (coefficients) of the logistic regression model. Common 
                        solvers include 'liblinear', 'newton-cg', 'lbfgs', 'sag', 
                        and 'saga'. You should try different solver values"
            from sklearn.linear_model import LogisticRegression
            from sklearn.metrics import confusion_matrix 
            LR = LogisticRegression(C=0.01, solver='liblinear').fit(X_train, y_train)
            LR
        6. Prediction:
            yhat = LR.predict(X_test)
            yhat 
        7. Accuracy Evaluation: jaccard index and confusion matrix 
          7.a jaccard index
            jaccard_score(y_test, yhat,pos_label=0)
          7.b.1 confusion matrix
                def plot_confusion_matrix(cm, classes,
                            normalize=False,
                            title='Confusion matrix',
                            cmap=plt.cm.Blues):
                    """
                    This function prints and plots the confusion matrix.
                    Normalization can be applied by setting `normalize=True`.
                    """
                    if normalize:
                        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
                        print("Normalized confusion matrix")
                    else:
                        print('Confusion matrix, without normalization')

                    print(cm)

                    plt.imshow(cm, interpolation='nearest', cmap=cmap)
                    plt.title(title)
                    plt.colorbar()
                    tick_marks = np.arange(len(classes))
                    plt.xticks(tick_marks, classes, rotation=45)
                    plt.yticks(tick_marks, classes)

                    fmt = '.2f' if normalize else 'd'
                    thresh = cm.max() / 2.
                    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
                        plt.text(j, i, format(cm[i, j], fmt),
                                horizontalalignment="center",
                                color="white" if cm[i, j] > thresh else "black")

                    plt.tight_layout()
                    plt.ylabel('True label')
                    plt.xlabel('Predicted label')
                print(confusion_matrix(y_test, yhat, labels=[1,0]))

          7.b.2 compute confusion matrix and plot non-normalized confusion matrix 
            import itertools
            from sklearn.metrics import confusion_matrix
            from sklearn import svm
            #compute confusion matrix
            cnf_matrix = confusion_matrix(y_test, yhat, labels=[1,0])
            np.set_printoptions(precision=2)

            #plot non-normalized confustion matrix 
            plt.figure()
            plot_confusion_matrix(cnf_matrix, 
                                    classes=['target_data_column=1', 'target_data_column=0'], 
                                    normalize=False, 
                                    title='Confusion Matrix')
          7.c Get Classification Report:
            from sklearn.metrics import classification_report
            print(classification_report(y_test, yhat))

          7.d log Loss  
            -measures  the performance of a classifier where the predicted output is a probability 
                    value between 0 and 1



    SVM(Support Vector Machines):

        offers a choice of kernel function for performing its processing. Basically mapping data into 
            higher dimensional space which is referred to as kernelling. The mathematical function used for 
            the transformation is known as the kernel function, and can be of diffrenent types.

                1. Linear
                2. Polynomial
                3. Radial basis function (RBF)
                4. sigmoid

        Each of the functions has its characteristics, its pros and cons, and its equation, but as there's
            no easy way of knowing which function performs best with any given dataset. We usually choose 
            different functions in turn and compare the results. 


        Steps:

        1. ETL:
            -make sure all cells are numerical 
        2a. Select independent variables:  
                feature_df = df[['column1', 'column2', 'column3', 'etc.']]
                #l convert feature_df to numpy array    
                X = np.asarray(feature_df)
                X[0:5]
        2b. Select dependent variable:
                # select column for the dependent variable
                df['dependent_var_column'] = df['dependent_var_column].asypte('int')

                # Convert the df to a numpy array
                y = np.asarray(df['dependent_var_column'])
        3. Train/Test dataset
                from sklearn.model_selection import train_test_split
                X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=4)
                print ('Train set:', X_train.shape,  y_train.shape)
                print('Test set:', X_test.shape,  y_test.shape)
        4. Modeling: SVM
            from sklearn import svm 
            clf = svm.SVC(kernel='rbf')
            clf.fit(X_train, y_train)
        5. Prediction: yhat 
            yhat = clf.predict(X_test)
            yhat[0:5]
        6.a Evaluation: Classification_Repoart and Confusion Matrix
            def plot_confusion_matrix(cm, classes,
                          normalize=False,
                          title='Confusion matrix',
                          cmap=plt.cm.Blues):
                """
                This function prints and plots the confusion matrix.
                Normalization can be applied by setting `normalize=True`.
                """
                if normalize:
                    cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
                    print("Normalized confusion matrix")
                else:
                    print('Confusion matrix, without normalization')

                print(cm)

                plt.imshow(cm, interpolation='nearest', cmap=cmap)
                plt.title(title)
                plt.colorbar()
                tick_marks = np.arange(len(classes))
                plt.xticks(tick_marks, classes, rotation=45)
                plt.yticks(tick_marks, classes)

                fmt = '.2f' if normalize else 'd'
                thresh = cm.max() / 2.
                for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
                    plt.text(j, i, format(cm[i, j], fmt),
                            horizontalalignment="center",
                            color="white" if cm[i, j] > thresh else "black")

                plt.tight_layout()
                plt.ylabel('True label')
                plt.xlabel('Predicted label')
        6.b
            from sklearn.metrics import classification_report, confusion_matrix
            # Compute confusion matrix
            cnf_matrix = confusion_matrix(y_test, yhat, labels=[2,4])
            np.set_printoptions(precision=2)

            print (classification_report(y_test, yhat))

            # Plot non-normalized confusion matrix
            plt.figure()
            plot_confusion_matrix(cnf_matrix, classes=['Benign(2)','Malignant(4)'],normalize= False,  title='Confusion matrix')

        6.optional
            from sklearn.metrics import f1_score
            f1_score(y_test, yhat, average='weighted')

            or 

            from sklearn.metrics import jaccard_score
            jaccard_score(y_test, yhat, pos_label=2)
        
