
Getting Started with Data Science:
Author: Murtaza Haider
"The other key ingredient for a successful data scientist is a behavioral trait: curiosity. A data scientist 
has to be one with a very curious mind, willing to spend significant time and effort to explore her hunches. 
In journalism, the editors call it having the nose for news. Not all reporters know where the news lies. Only
those Who have the nose for news get the Story. Curiosity is equally important for data scientists as it is for journalists."

Rachel Schutt is the Chief Data Scientist at News Corp. She teaches a data science course at Columbia University. 
She is also the author of an excellent book, Doing Data Science. In an interview With the New York Times, Dr Schutt 
defined a data scientist as someone who is a part computer scientist, part software engineer, and part statistician 
(Miller, 2013). But that’s the definition of an average data scientist. “The best”, she contended, “tend to be really 
curious people, thinkers who ask good questions and are O.K. dealing with unstructured situations and trying to find 
structure in them.”

How Big Data is defined by the Vs: Velocity, Volume, Variety, Veracity, and Value.
How Hadoop and other tools, combined with distributed computing power,  are used to handle the demands of Big Data.  
What skills are required to analyse Big Data. 
About the process of Data Mining, and how it produces results.

Big Data V's:
Velocity 
VolumeVariety 
Veracity 
Value

Distributive Analysis:
Apache Spark
Hadoop - Google big data architecture 
    mapper process/map 
    reduce process 

Digital Transformation 
    affects business operations 
    updating existing processes
    operations 
    creating new operations 

Deep Learning vs. Machine Learning:
    Data Mining 
        -process of automatically searching and analyzing data, discovering previously unrevealed patterns
    Machine Learning    
        -subset of AI that uses computer algorithms to analyze data and make 
            intelligent decisions based on what it has learned without being explicitly programmed 
        -traind with large sets of data and they learn from examples 
        - do not follow rules based algorithms 
        -enables machines to solve problems on their own and make accurte predictions using the provided data 
        -Netflix recommendations 
    Deep Learning 
        -Specialized subset of machine learning that uses layered neural networks to simulate human decision making 
        -algorithms can label and categorize information and identify patterns 
        -enables AI systems to continuously learn on the job and improve  the quality and accuracy of results by determining 
            whether decisions were correct 
        -Neural Networks collection of small computing units called neurons that take incomeing data and 
            learn to make decisions over time
            -ofter layer deep and are the reason deep learning algorithms become more efficient  as the data 
                sets increase in volumne, as opposed to opther machine learning algorithms  that may plateau as data increases

    ********************
    Data Science 
        -process and method for extacting knowledge and insights from large volumes 
            of disparate data. 
        -interdisciplinary field involving mathematics, statistical analysis, data vis., 
            machine learning and more 
        -makes it possible to appropriate information, see patterns, find meaning from large volumes of data 
            and use it to make decisions that drive business
        -broad term that encompasses the entire data processing methodology 
    AI 
        -everything that aloows computers to learn how to solve problems and make intelligent decisions

Neural Networks vs Deep Learning:
    Neural Network 
        -a computer program that will mimc how our brains use neurons to process
    Deep Learning   
        -neural networks on steroids
        -lots of linear algebra calculations 
            -transformations
        -High Powered Computational resources 
        -classifications
        -Bayesian Analysis 
        -have to understand what the meanings of predictive techniques 

Data Scientists need programming, mathematics, and database skills, many of which can be gained through self-learning.
Companies recruiting for a Data Science team need to understand the variety of different roles Data Scientists can play,
     and look for soft skills like storytelling and relationship building as well as technical skills.
High school students considering a career in Data Science should learn programming, math, databases, and, most importantly practice their skills.

The length and content of the final report will vary depending on the needs of the project.
The structure of the final report for a Data Science project should include a cover page, table of contents, 
    executive summary, detailed contents, acknowledgements, references and appendices.
The report should present a thorough analysis of the data and communicate the project findings

Reports:
Cover page
Table of contents
Executive Summary/Abstract 
methodology
Results 
Discussion 
Conclussion 
references
acknowledgements
appendices



**********************Toosl for Data Science**************************************

Data Science Categories 
    Data Management 
    Data Integration and Transformation
        -etl
    Data Visualization
    Model Building 
    Model Deployment 

Github is good code asset management DAM 
IBM Watson Studio IDE 

Data Science Tasks:
Data Asset Managment 
Code Asset Management 
Execution Environments 

Open Source Tools
Pixie Dust 
    -python library for data vis 
Jupyter
    the main difference between Jupyter Lab and Jupyter Notebooks is the ability to open different 
        types of files, including Jupyter Notebooks, data, and terminals, and then arrange them on the canvas.
    RStudio unifies programming, execution, debugging, remote data access, data exploration, and visualization into one tool.
RStudio 
    -R version of Jupyter
    -RStudio unifies programming, execution, debugging, remote data access, data exploration, and visualization into one tool.
Spyder 
    -Spyder tries to mimic the behavior of RStudio to bring its functionality to the Python world. Although not at par with the 
        functionality of RStudio, data scientists consider it as an alternative
        -Spyder tries to mimic the behavior of RStudio to bring its functionality to the Python world.
        -Spyder integrates code, documentation, and visualizations, among others, into a single canvas
Apache Spark 
    -cluster execution environment
    -most used across industries
    -linear scalability 
    -Apache Spark is a batch data processing engine, 
        capable of processing vast amounts of data one by one or file by file
Apache Flink 
    -was developed after Apache Spark continued to gain market share.
    -key difference between Apache Spark and Apache Flink is that Apache Spark is a batch data processing engine, 
        capable of processing vast amounts of data one by one or file by file
    - Apache Flink is a stream-processing image with its main focus on processing real-time data streams
KNIME 
    -originated from the University of Konstanz in 2004. As you can see, KNIME has a visual user interface with drag-and-drop capabilities. 
    -It has built-in visualization capabilities. 
    -can be extended by programming in R and Python and even has connectors to Apache Spark.
ORANGE 
    -another representative of this group of tools. It is less flexible than KNIME but is easier to use


Commercial Tools:
    most of an enterprise’s relevant data is stored in an Oracle Database, in Microsoft SQL Server, or an IBM Db2

    ETL TOOLS:
        Gartner Magic Quadrant, Informatica PowerCenter and IBM InfoSphere DataStage are the leaders. 
        These are followed by SAP, Oracle, SAS, Talend, and Microsoft products

    Data Vis:
        Cognos, Tableau, Power BI 

    Machine Learning:
        SPSS Modeler, 
            supports exporting models as predictive model markup language (PMML), which an abundance of other commercial and open software packages can read
        Watson Studio Desktop
Fairness Tools:
    Prometheus 
    IBM Research Trusted AI 
        -AI Fariness 360 Open Source Toolkit 

Cloud based Data management:
    Amazon DynamoDB
    Cloudant 
    CouchDB relax
    IBM Db2

    cloud data integration and transformation 
        Informatica
        IBM Data Refinery

    cloud data vis:
        Datameer 
            =small
        IBM cOGNOS ANALYTICS
        IBM Watson Studio 
            -3d bar chart
                visualize target value on the vertical dimension 
                    dependent on two other values in the horizontal dimensions 
            -hierarchical edge bundling 
                depicts correlations and affliations between entites 

    cloud model building 
        IBM Wateson Machine learning 
        Google - AI Platform Training 
        Amazon SAge 

    cloud full scale complete development life cycle for all data science, machine learning, and AI tasks 
        -Watson STudio 
        -Watson OpenScale

Data management tools are MySQL, PostgreSQL, MongoDB, Apache CouchDB, Apache Cassandra, Hadoop File System, Ceph, and elastic search.
Data integration and transformation tools are Apache AirFlow, KubeFlow, Apache Kafka, Apache Nifi, Apache SparkSQL, and NodeRED
Data Visualization tools are Pixie Dust, Hue, Kibana, and Apache Superset
Model deployment tools are Apache PredictionIO, Seldon, Kubernetes, Redhat OpenShift, Mleap, TensorFlow service, TensorFlow lite, and TensorFlow dot JS
Model monitoring tools are ModelDB, Prometheus, IBM AI Fairness 360, IBM Adversarial Robustness 360 Toolbox, and IBM AI Explainability 360.
Code asset management tools are Git, GitHub, GitLab, and Bitbucket
data asset management tools are Apache Atlas, ODPi Egeria, and Kylo.
    Data governance
    data versioned and annotated
    data dictionary
    data lineage
    data privacy retention 

 Information Governance Catalog 
    covers functions like a data dictionary, which facilitates the discovery of data assets. 
    Each data asset is assigned to a data steward or the data owner. 
    The data owner is responsible for that data asset and can be contacted. 
    Then, data lineage is covered, allowing tracking back the transformation steps in creating the data assets. 
    The data lineage also includes a reference to the actual source data. 
    Rules and policies can be added to reflect complex regulatory and business requirements for data privacy and retention. 

    The Data Science Task Categories include:

Data Management -  storage, management and retrieval of data

Data Integration and Transformation - streamline data pipelines and automate data processing tasks

Data Visualization - provide graphical representation of data and assist with communicating insights

Modelling - enable Building, Deployment, Monitoring and Assessment of Data and Machine Learning models

Data Science Tasks support the following:

Code Asset Management - store & manage code, track changes and allow collaborative development

Data Asset Management - organize and manage data, provide access control, and backup assets

Development Environments - develop, test and deploy code

Execution Environments - provide computational resources and run the code

The data science ecosystem consists of many open source and commercial options, and include both traditional desktop applications and server-based tools, as well as cloud-based services that can be accessed using web-browsers and mobile interfaces.

Data Management Tools: include Relational Databases, NoSQL Databases, and Big Data platforms:

MySQL, and PostgreSQL are examples of Open Source Relational Database Management Systems (RDBMS), and IBM Db2 and SQL Server are examples of commercial RDBMSes and are also available as Cloud services.

MongoDB and Apache Cassandra are examples of NoSQL databases.

Apache Hadoop and Apache Spark are used for Big Data analytics. 

Data Integration and Transformation Tools: include Apache Airflow and Apache Kafka. 

Data Visualization Tools:  include commercial offerings  such as Cognos Analytics, Tableau and PowerBI  and can be used for building dynamic and interactive dashboards.  

Code Asset Management Tools: Git is an essential code asset management tool. GitHub is a popular web-based platform for storing and managing source code. Its features make it an ideal tool for collaborative software development, including version control, issue tracking, and project management. 

Development Environments: Popular development environments for Data Science include Jupyter Notebooks and RStudio. 

Jupyter Notebooks provides an interactive environment for creating and sharing code, descriptive text, data visualizations, and other computational artifacts in a web-browser based interface.  

RStudio is an integrated development environment (IDE) designed specifically for working with 



Languages of Data Science:
     popular languages are Python, R, SQL, Scala, Java, C++, and Julia. JavaScript, PHP, Go, Ruby, and Visual Basic all have their own unique use cases as well
    Open source is more business focused, while free software is more focused on a set of values. 


    Python 
        3/4 of data science jobs use python 

    R
        Now, let’s discuss the differences between open source and free software. The Open-Source Initiative (OSI) champions open source, while the Free Software 
            Foundation (FSF) defines free software. Open source is more business focused, while free software is more focused on a set of values. 
            So, why R? You should learn R because it is free software
        R 
            Language's array-oriented syntax makes it easier to translate from math to code for learners with no or minimal programming background
                R is mostly popular in academia. In addition, companies that use R include IBM, Google, Facebook, Microsoft, Bank of America, Ford, TechCrunch, Uber, and 
                Trulia. R has become the world’s largest repository of statistical knowledge. 
            As of 2018, R has more than 15,000 publicly released packages making it possible to conduct complex exploratory data analysis. 
                R integrates well with other computer languages like C++, Java, C, .Net, and Python. Using R, common mathematical operations like matrix 
                multiplication give immediate results. And R has stronger object-oriented programming facilities than most statistical computing languages.

The Open-Source Initiative (OSI) champions open source, while the Free Software Foundation (FSF) defines free software. Python is open source, and R is free 
    software. R language’s array-oriented syntax makes it easier to translate from math to code for learners with none or minimal 
    programming background. And R has become the world’s largest repository of statistical knowledge.
        
Data Science Roles 
    Business Analyst, Database Engineer, Data Analyst, Data Engineer, Data Scientist, Research Scientist, Software Engineer, Statistician, Product Manager, Project Manager, 


SQL:
    Language elements 
     Clauses, Expressions, Predicates, Queries, and Statements.

Java 
    -object oriented programming language 

    Hadoop  
        cluster systems 

Scala   

    Apache Spark 
    designed 

C++
    general purpose language
    extension of Cassandra
    -improves processing speed, 
    -enable ssytem programming 
    -broader application control 

    Tensorflow 
        -built on top of C++ 
        -Python interface 

    MongoDB
        -C++ application Nosql db 

    Caffe
        -deep learning algorithm repository 
        -C++ with Python and matlab bindings  
    -Caffe supports many different types of deep learning architectures geared towards image classification and image segmentation. 
        It supports CNN, RCNN, LSTM and fully-connected neural network designs.[8] Caffe supports GPU- and CPU-based acceleration 
        computational kernel libraries such as Nvidia cuDNN and Intel MKL.[9][10]

Julia 
    Julia was designed at MIT for high-performance numerical analysis and computational science. Julia provides speedy development 
        like Python or R, while producing programs that run as fast as C or Fortran programs. It’s compiled which means that Julia 
        code is executed directly on the processor as executable code. It calls C, Go, Java, MATLAB, R, Fortran, and Python libraries, 
        and has refined parallelism. Julia as a language is only 8 years old, written in 2012, but there is a lot of promise for its 
        future impact on the data science industry. One great application of Julia for Data Science is JuliaDB, which is a package for 
        working with large persistent data sets


Libraries for Data Science 
    PYthon libraries 
        Pandas
            -built on top of numpy
        Numpy
            arrays 
            matrixs
        
        Data Vis    
            Matplotlib
            Seaborn
        
        Machine Learning    
            Scikit learn    
                regression
                classification
                clustering
            Keras
                Deep Learning Nearal Networks
                
        Deep Learning 
            TensorFlow  
                Production and Deployment 
                low level frame work 
            PyTorch
                Deep Learning: 
                    regression, classification 

        Cluster Computing 
            Apache Spark 
                functional with pandas, numpy, scikit-learn
                pYTHON R Scala SQL 

    Scala   
        Vegas
            data Vis
        Big DL
            deep learning 

    R 
        ggplot2 
            data vis 
        other libraries that allow you to interface with keras and tensor flow
        -was defacto data science tool but no longer 

API 
    REST 
        representational state transfer 
        used for interacting with web services 
            client - me 
            resource 
            REST api
        http
            response 
            json
    
    Watson text to speech API 
        client to Watson Text to speech 

    Watson Language-translator API 


Data Sets 
    which of the following is open data?
        government data 
    which license stipulates that  the modified version of the data should be published under the same license 
        terms as the original data?
            CDLA-sharing

    IBM Model Asset Exchange 
        data
            research 
            Model 
            Compute Resources 
            Domain Expertise 
        MAX 
            free open source repository for ready to use, customizable deep learning microservices
                pre or custom trainable deep learning models 
                fully tested can be deployed in minutes 
                object detection 
                image, audio videio 
                named entity recognition 
                image to text translation 
                human pose detection 
            typical model serving microservices
                pre-trained deep learning model 
                code to preprocess input 
                code to post process output 
                public api to connect apps 
                How do they work?
                    input/output/model-processing code
                    validated model (code)
                        implement
                        package
                        document 
                        test 
                    REST API 
    

        https://www.ml-exchange.org/models 



    IBM Data Asset eXchange DAX
        developer.ibm.com/

Machine Learning Models:
    ML uses models to identify patterns in data 
    Model training is the process by which the model learns the data patterns 
    after training it can be used to make predictions 

    ML Models:
        Supervised
            identifies relationships and dependencies between the input data and correct output
                regression 
                    -predicts real numberical values 
                        home sale prices 
                classification 
                    -predicts data categories 
                        email spam filters, fraud detection 
        Unsupervised
            data is not labeled 
            model tries to identify patterns without external help 
                clustering divides each record of a dataset into one of similar group 
                anomaly detection identifies outliers in a dataset 
        Reinforcement
            conceputally similar to human learning process  
            learning through trial and error 
        
    Deep Learning   
        tries to loosely emulate how the human brain works 
        applications    
            natural language processing 
            image, audio and video analysis 
            time series forecasting 
        requires large datasets of labeled data and is compute intensive 
        requires special purpose hardware 
        can be built from scratch or public model repositories 
        built using frameworks/libraries    
            TensorFlow
            PyTorch
            Keras
        provide Python api and support c++ and javascript 
        popular model repositories 
            most frameworks provides a model zoo 
                TensorFlow, Pytorch, keras and ONNX model zoos 
        
        How to build a model 
            Images Deep Learning 
                prepare data
                build model 
                    scratch or pre existing 
                train model 
                deploy model 
                use model 










Government Data:

https://www.data.gov/
https://www.census.gov/data.html
https://data.gov.uk/
https://www.opendatanetwork.com/
https://data.un.org/
Financial Data Sources:

https://data.worldbank.org/
https://www.globalfinancialdata.com/
https://comtrade.un.org/
https://www.nber.org/
https://fred.stlouisfed.org/
Crime Data:

https://www.fbi.gov/services/cjis/ucr
https://www.icpsr.umich.edu/icpsrweb/content/NACJD/index.html
https://www.drugabuse.gov/related-topics/trends-statistics
https://www.unodc.org/unodc/en/data-and-analysis/
Health Data:

https://www.who.int/gho/database/en/
https://www.fda.gov/Food/default.htm
https://seer.cancer.gov/faststats/selections.php?series=cancer
https://www.opensciencedatacloud.org/
https://pds.nasa.gov/
https://earthdata.nasa.gov/
https://www.sgim.org/communities/research/dataset-compendium/public-datasets-topic-grid
Academic and Business Data:

https://scholar.google.com/
https://nces.ed.gov/
https://www.glassdoor.com/research/
https://www.yelp.com/dataset
Other General Data:

https://www.kaggle.com/datasets
https://www.reddit.com/r/datasets/

Dataset licenses
When you select a dataset, it is necessary to look into the license. A license explains whether you can use that dataset or not; or explains if you have to accept certain guidelines to use that dataset. The different license types are listed below.

PUBLIC DOMAIN MARK - PUBLIC DOMAIN
When a dataset has a Public Domain license, all the rights to use, access, modify and share the dataset are open to everyone. Here there is technically no license.

OPEN DATA COMMONS PUBLIC DOMAIN DEDICATION AND LICENSE – PDDL
Open Data Commons license has the same features as the Public Domain license, but the difference is the PDDL license uses a licensing mechanism to give the rights to the dataset.

CREATIVE COMMONS ATTRIBUTION 4.0 INTERNATIONAL CC-BY
This license allows users to share and modify a dataset, but only if they give credit to the creator(s) of the dataset.

COMMUNITY DATA LICENSE AGREEMENT – CDLA PERMISSIVE-2.0
Like most open-source licenses, this license allows users to use, modify, adapt, and share the dataset, but only if a disclaimer of warranties and liability is also included.

OPEN DATA COMMONS ATTRIBUTION LICENSE - ODC-BY
This license allows users to share and adapt a dataset, but only if they give credit to the creator(s) of the dataset.

CREATIVE COMMONS ATTRIBUTION-SHAREALIKE 4.0 INTERNATIONAL - CC-BY-SA
This license allows users to use, share, and adapt a dataset, but only if they give credit to the dataset and show any changes or transformations, they made to the dataset. Users might not want to use this license because they have to share the work they did on the dataset.

COMMUNITY DATA LICENSE AGREEMENT – CDLA-SHARING-1.0
This license uses the principle of ‘copyleft’: users can use, modify, and adapt a dataset, but only if they don’t add license restrictions on the new work(s) they create with the dataset.

OPEN DATA COMMONS OPEN DATABASE LICENSE - ODC-ODBL
This license allows users to use, share, and adapt a dataset but only if they give credit to the dataset and show any changes or transformations they make to the dataset. Users might not want to use this license because they have to share the work they did on the dataset.

CREATIVE COMMONS ATTRIBUTION-NONCOMMERCIAL 4.0 INTERNATIONAL - CC BY-NC
This license is a restrictive license. Users can share and adapt a dataset, provided they give credit to its creator(s) and ensure that the dataset is not used for any commercial purpose.

CREATIVE COMMONS ATTRIBUTION-NO DERIVATIVES 4.0 INTERNATIONAL - CC BY-ND
This license is also a restrictive license. Users can share a dataset if they give credit to its creator(s). This license does not allow additions, transformations, or changes to the dataset.

CREATIVE COMMONS ATTRIBUTION-NONCOMMERCIAL-SHAREALIKE 4.0 INTERNATIONAL - CC BY-NC-SA
This license allows users to share a dataset only if they give credit to its creator(s). Users can share additions, transformations, or changes to the dataset, but they cannot use the dataset for commercial purposes.

CREATIVE COMMONS ATTRIBUTION-NONCOMMERCIAL-NODERIVATIVES 4.0 INTERNATIONAL - CC BY-NC-ND
This license allows users to share a dataset only if they give credit to its creator(s). Users are not allowed to modify the dataset and are not allowed to use it for commercial purposes.


