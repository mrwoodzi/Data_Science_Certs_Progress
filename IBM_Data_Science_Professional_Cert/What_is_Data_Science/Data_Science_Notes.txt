
Getting Started with Data Science:
Author: Murtaza Haider
"The other key ingredient for a successful data scientist is a behavioral trait: curiosity. A data scientist 
has to be one with a very curious mind, willing to spend significant time and effort to explore her hunches. 
In journalism, the editors call it having the nose for news. Not all reporters know where the news lies. Only
those Who have the nose for news get the Story. Curiosity is equally important for data scientists as it is for journalists."

Rachel Schutt is the Chief Data Scientist at News Corp. She teaches a data science course at Columbia University. 
She is also the author of an excellent book, Doing Data Science. In an interview With the New York Times, Dr Schutt 
defined a data scientist as someone who is a part computer scientist, part software engineer, and part statistician 
(Miller, 2013). But that’s the definition of an average data scientist. “The best”, she contended, “tend to be really 
curious people, thinkers who ask good questions and are O.K. dealing with unstructured situations and trying to find 
structure in them.”

How Big Data is defined by the Vs: Velocity, Volume, Variety, Veracity, and Value.
How Hadoop and other tools, combined with distributed computing power,  are used to handle the demands of Big Data.  
What skills are required to analyse Big Data. 
About the process of Data Mining, and how it produces results.

Big Data V's:
Velocity 
VolumeVariety 
Veracity 
Value

Distributive Analysis:
Apache Spark
Hadoop - Google big data architecture 
    mapper process/map 
    reduce process 

Digital Transformation 
    affects business operations 
    updating existing processes
    operations 
    creating new operations 

Deep Learning vs. Machine Learning:
    Data Mining 
        -process of automatically searching and analyzing data, discovering previously unrevealed patterns
    Machine Learning    
        -subset of AI that uses computer algorithms to analyze data and make 
            intelligent decisions based on what it has learned without being explicitly programmed 
        -traind with large sets of data and they learn from examples 
        - do not follow rules based algorithms 
        -enables machines to solve problems on their own and make accurte predictions using the provided data 
        -Netflix recommendations 
    Deep Learning 
        -Specialized subset of machine learning that uses layered neural networks to simulate human decision making 
        -algorithms can label and categorize information and identify patterns 
        -enables AI systems to continuously learn on the job and improve  the quality and accuracy of results by determining 
            whether decisions were correct 
        -Neural Networks collection of small computing units called neurons that take incomeing data and 
            learn to make decisions over time
            -ofter layer deep and are the reason deep learning algorithms become more efficient  as the data 
                sets increase in volumne, as opposed to opther machine learning algorithms  that may plateau as data increases

    ********************
    Data Science 
        -process and method for extacting knowledge and insights from large volumes 
            of disparate data. 
        -interdisciplinary field involving mathematics, statistical analysis, data vis., 
            machine learning and more 
        -makes it possible to appropriate information, see patterns, find meaning from large volumes of data 
            and use it to make decisions that drive business
        -broad term that encompasses the entire data processing methodology 
    AI 
        -everything that aloows computers to learn how to solve problems and make intelligent decisions

Neural Networks vs Deep Learning:
    Neural Network 
        -a computer program that will mimc how our brains use neurons to process
    Deep Learning   
        -neural networks on steroids
        -lots of linear algebra calculations 
            -transformations
        -High Powered Computational resources 
        -classifications
        -Bayesian Analysis 
        -have to understand what the meanings of predictive techniques 

Data Scientists need programming, mathematics, and database skills, many of which can be gained through self-learning.
Companies recruiting for a Data Science team need to understand the variety of different roles Data Scientists can play,
     and look for soft skills like storytelling and relationship building as well as technical skills.
High school students considering a career in Data Science should learn programming, math, databases, and, most importantly practice their skills.

The length and content of the final report will vary depending on the needs of the project.
The structure of the final report for a Data Science project should include a cover page, table of contents, 
    executive summary, detailed contents, acknowledgements, references and appendices.
The report should present a thorough analysis of the data and communicate the project findings

Reports:
Cover page
Table of contents
Executive Summary/Abstract 
methodology
Results 
Discussion 
Conclussion 
references
acknowledgements
appendices



**********************Toosl for Data Science**************************************

Data Science Categories 
    Data Management 
    Data Integration and Transformation
        -etl
    Data Visualization
    Model Building 
    Model Deployment 

Github is good code asset management DAM 
IBM Watson Studio IDE 

Data Science Tasks:
Data Asset Managment 
Code Asset Management 
Execution Environments 

Open Source Tools
Pixie Dust 
    -python library for data vis 
Jupyter
    the main difference between Jupyter Lab and Jupyter Notebooks is the ability to open different 
        types of files, including Jupyter Notebooks, data, and terminals, and then arrange them on the canvas.
    RStudio unifies programming, execution, debugging, remote data access, data exploration, and visualization into one tool.
RStudio 
    -R version of Jupyter
    -RStudio unifies programming, execution, debugging, remote data access, data exploration, and visualization into one tool.
Spyder 
    -Spyder tries to mimic the behavior of RStudio to bring its functionality to the Python world. Although not at par with the 
        functionality of RStudio, data scientists consider it as an alternative
        -Spyder tries to mimic the behavior of RStudio to bring its functionality to the Python world.
        -Spyder integrates code, documentation, and visualizations, among others, into a single canvas
Apache Spark 
    -cluster execution environment
    -most used across industries
    -linear scalability 
    -Apache Spark is a batch data processing engine, 
        capable of processing vast amounts of data one by one or file by file
Apache Flink 
    -was developed after Apache Spark continued to gain market share.
    -key difference between Apache Spark and Apache Flink is that Apache Spark is a batch data processing engine, 
        capable of processing vast amounts of data one by one or file by file
    - Apache Flink is a stream-processing image with its main focus on processing real-time data streams
KNIME 
    -originated from the University of Konstanz in 2004. As you can see, KNIME has a visual user interface with drag-and-drop capabilities. 
    -It has built-in visualization capabilities. 
    -can be extended by programming in R and Python and even has connectors to Apache Spark.
ORANGE 
    -another representative of this group of tools. It is less flexible than KNIME but is easier to use


Commercial Tools:
    most of an enterprise’s relevant data is stored in an Oracle Database, in Microsoft SQL Server, or an IBM Db2

    ETL TOOLS:
        Gartner Magic Quadrant, Informatica PowerCenter and IBM InfoSphere DataStage are the leaders. 
        These are followed by SAP, Oracle, SAS, Talend, and Microsoft products

    Data Vis:
        Cognos, Tableau, Power BI 

    Machine Learning:
        SPSS Modeler, 
            supports exporting models as predictive model markup language (PMML), which an abundance of other commercial and open software packages can read
        Watson Studio Desktop
Fairness Tools:
    Prometheus 
    IBM Research Trusted AI 
        -AI Fariness 360 Open Source Toolkit 

Cloud based Data management:
    Amazon DynamoDB
    Cloudant 
    CouchDB relax
    IBM Db2

    cloud data integration and transformation 
        Informatica
        IBM Data Refinery

    cloud data vis:
        Datameer 
            =small
        IBM cOGNOS ANALYTICS
        IBM Watson Studio 
            -3d bar chart
                visualize target value on the vertical dimension 
                    dependent on two other values in the horizontal dimensions 
            -hierarchical edge bundling 
                depicts correlations and affliations between entites 

    cloud model building 
        IBM Wateson Machine learning 
        Google - AI Platform Training 
        Amazon SAge 

    cloud full scale complete development life cycle for all data science, machine learning, and AI tasks 
        -Watson STudio 
        -Watson OpenScale

Data management tools are MySQL, PostgreSQL, MongoDB, Apache CouchDB, Apache Cassandra, Hadoop File System, Ceph, and elastic search.
Data integration and transformation tools are Apache AirFlow, KubeFlow, Apache Kafka, Apache Nifi, Apache SparkSQL, and NodeRED
Data Visualization tools are Pixie Dust, Hue, Kibana, and Apache Superset
Model deployment tools are Apache PredictionIO, Seldon, Kubernetes, Redhat OpenShift, Mleap, TensorFlow service, TensorFlow lite, and TensorFlow dot JS
Model monitoring tools are ModelDB, Prometheus, IBM AI Fairness 360, IBM Adversarial Robustness 360 Toolbox, and IBM AI Explainability 360.
Code asset management tools are Git, GitHub, GitLab, and Bitbucket
data asset management tools are Apache Atlas, ODPi Egeria, and Kylo.
    Data governance
    data versioned and annotated
    data dictionary
    data lineage
    data privacy retention 

 Information Governance Catalog 
    covers functions like a data dictionary, which facilitates the discovery of data assets. 
    Each data asset is assigned to a data steward or the data owner. 
    The data owner is responsible for that data asset and can be contacted. 
    Then, data lineage is covered, allowing tracking back the transformation steps in creating the data assets. 
    The data lineage also includes a reference to the actual source data. 
    Rules and policies can be added to reflect complex regulatory and business requirements for data privacy and retention. 

    The Data Science Task Categories include:

Data Management -  storage, management and retrieval of data

Data Integration and Transformation - streamline data pipelines and automate data processing tasks

Data Visualization - provide graphical representation of data and assist with communicating insights

Modelling - enable Building, Deployment, Monitoring and Assessment of Data and Machine Learning models

Data Science Tasks support the following:

Code Asset Management - store & manage code, track changes and allow collaborative development

Data Asset Management - organize and manage data, provide access control, and backup assets

Development Environments - develop, test and deploy code

Execution Environments - provide computational resources and run the code

The data science ecosystem consists of many open source and commercial options, and include both traditional desktop applications and server-based tools, as well as cloud-based services that can be accessed using web-browsers and mobile interfaces.

Data Management Tools: include Relational Databases, NoSQL Databases, and Big Data platforms:

MySQL, and PostgreSQL are examples of Open Source Relational Database Management Systems (RDBMS), and IBM Db2 and SQL Server are examples of commercial RDBMSes and are also available as Cloud services.

MongoDB and Apache Cassandra are examples of NoSQL databases.

Apache Hadoop and Apache Spark are used for Big Data analytics. 

Data Integration and Transformation Tools: include Apache Airflow and Apache Kafka. 

Data Visualization Tools:  include commercial offerings  such as Cognos Analytics, Tableau and PowerBI  and can be used for building dynamic and interactive dashboards.  

Code Asset Management Tools: Git is an essential code asset management tool. GitHub is a popular web-based platform for storing and managing source code. Its features make it an ideal tool for collaborative software development, including version control, issue tracking, and project management. 

Development Environments: Popular development environments for Data Science include Jupyter Notebooks and RStudio. 

Jupyter Notebooks provides an interactive environment for creating and sharing code, descriptive text, data visualizations, and other computational artifacts in a web-browser based interface.  

RStudio is an integrated development environment (IDE) designed specifically for working with 



Languages of Data Science:
     popular languages are Python, R, SQL, Scala, Java, C++, and Julia. JavaScript, PHP, Go, Ruby, and Visual Basic all have their own unique use cases as well
    Open source is more business focused, while free software is more focused on a set of values. 


    Python 
        3/4 of data science jobs use python 

    R
        Now, let’s discuss the differences between open source and free software. The Open-Source Initiative (OSI) champions open source, while the Free Software 
            Foundation (FSF) defines free software. Open source is more business focused, while free software is more focused on a set of values. 
            So, why R? You should learn R because it is free software
        R 
            Language's array-oriented syntax makes it easier to translate from math to code for learners with no or minimal programming background
                R is mostly popular in academia. In addition, companies that use R include IBM, Google, Facebook, Microsoft, Bank of America, Ford, TechCrunch, Uber, and 
                Trulia. R has become the world’s largest repository of statistical knowledge. 
            As of 2018, R has more than 15,000 publicly released packages making it possible to conduct complex exploratory data analysis. 
                R integrates well with other computer languages like C++, Java, C, .Net, and Python. Using R, common mathematical operations like matrix 
                multiplication give immediate results. And R has stronger object-oriented programming facilities than most statistical computing languages.

The Open-Source Initiative (OSI) champions open source, while the Free Software Foundation (FSF) defines free software. Python is open source, and R is free 
    software. R language’s array-oriented syntax makes it easier to translate from math to code for learners with none or minimal 
    programming background. And R has become the world’s largest repository of statistical knowledge.
        
Data Science Roles 
    Business Analyst, Database Engineer, Data Analyst, Data Engineer, Data Scientist, Research Scientist, Software Engineer, Statistician, Product Manager, Project Manager, 


SQL:
    Language elements 
     Clauses, Expressions, Predicates, Queries, and Statements.

Java 
    -object oriented programming language 

    Hadoop  
        cluster systems 

Scala   

    Apache Spark 
    designed 

C++
    general purpose language
    extension of Cassandra
    -improves processing speed, 
    -enable ssytem programming 
    -broader application control 

    Tensorflow 
        -built on top of C++ 
        -Python interface 

    MongoDB
        -C++ application Nosql db 

    Caffe
        -deep learning algorithm repository 
        -C++ with Python and matlab bindings  
    -Caffe supports many different types of deep learning architectures geared towards image classification and image segmentation. 
        It supports CNN, RCNN, LSTM and fully-connected neural network designs.[8] Caffe supports GPU- and CPU-based acceleration 
        computational kernel libraries such as Nvidia cuDNN and Intel MKL.[9][10]

Julia 
    Julia was designed at MIT for high-performance numerical analysis and computational science. Julia provides speedy development 
        like Python or R, while producing programs that run as fast as C or Fortran programs. It’s compiled which means that Julia 
        code is executed directly on the processor as executable code. It calls C, Go, Java, MATLAB, R, Fortran, and Python libraries, 
        and has refined parallelism. Julia as a language is only 8 years old, written in 2012, but there is a lot of promise for its 
        future impact on the data science industry. One great application of Julia for Data Science is JuliaDB, which is a package for 
        working with large persistent data sets


Libraries for Data Science 
    