Different Components of a modern data ecosystem:

Describe and differentiate between the role different data professionals play in a modern data ecosystem:

Explain what data analysis is, the different types of data analysis, and the key steps in the data analysis process.

Describe the responsibilities and skillsets of a Data Analyst

Summarize a typical day in the life of a Data Analyst.

Fundamentals of data analysis:
-data gathering
-data wrangling
-mining
-analysis
-data visualization

Modern Data ecosystem:
-pull data from source and put into new database
    -reliability, security and integrity of data at this stage
-organize, clean and optimize, conform to guidelines
    -adhere to master data tables
    -data management, data repositories
-busines stake holders, apps, prog Analyst

Big Data

Data ecosystem:
- data holds key to competitive advantage
data engineer - convert raw data into usable table
    develop and maintain databases, extract, integrate, and organize data from disparate sources
    clean transform and prepare data
    design store and mangage data in repositories
data Analyst: use data to generate insights
    transforms data into plain English
    inspect and clean data
    find patterns 
    statistics
    visualize data
    answer questions, co-relation
data scientist - predict future using data from the past
    analyze data for actionable insights
    create predictive models using machine learning and deep learning
    answer questions like 'How many new social media followers am I likely to get next month.'
    or "What percentage of my customers am I likely to lose to competition in the next quarter."
    or "Is this 
business analyist and bi analyist - use all the above to gain insights and predictions
    leverage work of data analyist and data scientist
    focus on market forces and external influences that shape their business
    organize and monitor data on different business functions
    explore data to extract insights and actionables that improve business performance

Data Analysis:
    Recognizing patterns and correlations in data
        -insghts and conclusion drawn
        -helps to validate a course of actionable

4 types of data analysis:

        Descriptive Analytics
            -what happened in the past
        Diagnostic Analytics
            -why did it happened
        Predictive Analytics
            -predict future outcomes, what might happene
        Prescriptive Analytics
            -what should be done


How to think about analysis:
    Understand the problem and the desired result
    Setting a clear metric, what or how it will be measured
    Gathering data, indentify whats the best tool
    Cleaning data, fix issues in data that could effect accuracy, outliers, standardize data
    Anlyze and Mine data, indentify correlations
    Interpreting results, is analysis defendable
    Presenting your findings

    Also:
        Define problem
        Creating Hypothesis
        Collect and Clean Data
        Analyzing Data
        Presenting to Stakeholders

Defining Data Analysis:
    What is it- 
        Collecting Information - Use of Information
        Analyzing Data
        Confirming Hypothesis
        Storytelling with Data

***Difference between Analysis and Analytics:
        -Analysis - detailed examination of the elements or structure of something, can be done without numbers or data
        -Analytics - the systematic computational analysis of data or statistics


Module 2:

Responsibilities of a Data Analyst:
    -creating queries to extract required data
    -filtering, cleaning, standardizing and reorganizing data
    -using statistical tools and techniques
    -anaylyze patterns
    -prepare repositories
    -appropriate docs

Sklls - 
    -Technical
    -Functional tools   
        -statistics
        -analytical skills
        -problem solving skills
        -probing skills
        -data visualization
        -project management
    -Soft Skills
        -Work Collaboratively
        -Comm effectively
        -story telling
        -curiosity
        -intuition - ability to have sense of future

Day in the life of Data Analyst:
    -Asquiring data
    -Create queries
    -looking for insights
    -updating dashboards and reports 
    -interacting with Stakeholders for gathering Information and presenting findings 
    -clean and prepare data for analysis

    Initial Hypothesis  
    Identifying Datasets
    Find potential issues
    Present to stakeholders and also present the methods used to get to solution

Application of Data Analytics   
    -used by companies to identiy what information consumers want them to share
    -its everywhere 
    -widely applicable across industries, verticals and functions within
    -used to pay close attention to changes in customers buying habits

The role of a Data Analyst spans across:

    Acquiring data that best serves the use case.

    Preparing and analyzing data to understand what it represents.

    Interpreting and effectively communicating the message to stakeholders who need to act on the findings.

    Ensuring that the process is documented for future reference and repeatability.  

In order to play this role successfully, Data Analysts need a mix of technical, functional, and soft skills.  

    Technical Skills include varying levels of proficiency in using spreadsheets, statistical tools, visualization tools, 
    programming and querying languages, and the ability to work with different types of data repositories and big data platforms.

    An understanding of Statistics, Analytical techniques, problem-solving, the ability to probe a situation from multiple 
    perspectives, data visualization, and project management skills â€“ all of which come under Functional Skills a Data Analyst 
    needs in order to play an effective role.

    Soft Skills include the ability to work collaboratively, communicate effectively, tell a compelling story with data, and 
    garner support and buy-in from stakeholders. Curiosity to explore different pathways and intuition that helps to give a 
    sense of the future based on past experiences are also essential skills for being a good Data Analyst.  


Random:
IoT - Internet of Things
Data mart 
Data Warehouse 
Data Lake
Data Pipelines

Data analyist Ecosystem 
    -infrastructure, software, tools, frameworks, and processes to 
        -gather, clean, mine visualize
    
    Data    
        Structured - neat, rows and columns, excel
        Semi-Structured - mix of data that has consistent characteristics and data that does not conform to a rigid structure, email
        Unstructured - complex data that is qualitative that cannot be structured into rows and columns, photos video, text and social media content

        Data can come from 
            Relational database and non rdbs
            API, Web Services, Data Streams
            Social Platforms and Sensor Devices

        Data repositories:
            databases
            data warehouses
            data marts
            data lakes
            big data stores 
                large value high velocity is needed

                    The type of data, file formats and sources of data help determine which type of repositories 

        languages:
            Query languages
            Programming languages
            Shell and Scripting Languages 

        Automated Tools, frameworks and processes for all stages of the analytics process are part of the Data Analyst ecosystem

    Types of Data:
        Facts, Oberservations, Perceptions
        Numbers, Characters, Images

        Categories of Data:
            Structured, -well defined structure, stored in well defined schemas, represecnted in a tabular manner with rows 
                and columnas, sql database, online transactions, spreadsheets, online forms, sensors, network and webserver logs
            Semi-structured, -some organizational properties, not in rows and columns, contains metadata****, emails
                xml, binary executables, json, 
            Unstructured -Not easily identifired structure, not in rows or columns, 
                web pages, social media, baried images, video/audio, powerpoint, media logs, 
                files/docs, nosql, manual analysis 

        Different Types of File Structures:
            Delimited text file formats, .CSV-stores data as text, value separated by delimiter
                -delimiter - a sequence of one or more characters for specifiying the boundary between independent
                    entites or values
                     comma, tab, colon, vertical bar, space
                     comma-separated values, csv and tab-separated values, tsv
            Microsoft Excel Open, .xml spreadsheet or .xlsx, mulitple worksheets, rows and columns, open file format m
                more swecure
            Extensible Markup Language, .xml - set rules for encoding data
                language readable by humans and machines\
                self-descriptive language
                similar to .html, but does not use predefined tags
                platform independent
                programming language independent
                makes it simpler to share data between systems
            Portable Document Format, .pdf - developed by adobe to present documents independent of app, software, hardware
                 or os, can be viewed same way on any device
                 frequently used in legal and financial data
            JavaScript Object Notation, .json - test-based open standard
                designed for transmitting structured data over the web
                language-independent data format 
                read in any programming language
                compatible with wide range of browser
                considered as one of the best tools for sharing data

    Sources of Data:
        Relational databases - internal apps, sql server, oracle , ibm db2
            customers sales . . . could be public or private demographic, point of sale , weather data
            define strategy, predict demand, distribution/marketing decisions
            flat files, spreadsheet, xml doc
        Flat files and xml Datasets - plain text separated by delimiters, maps to single table, csv tsv
            spreadsheet files are flat files    
                tabular format rows and columns, worksheets, 
                xls, xlxs, apple office, libra office   
                    uses tabs, can be complex, hierarchical, Unstructured or sturctureed
        api and web services - Listen for incoming requests
            web or network requests 
            return plain text, json, or media files
            Ex. twitter or facebok api, stock market api, data lookup and validation api 
        web scraping - used to extract data from Unstructured   
            also known as screen scraping, web harvesting, web data extraction
            downloads specific data based on defined parameters
            -can extract text, images, videos, product items, 
            -Ex. collecting product details
                -sales leads
                -data from posts and autorh
                -data sets for API
            Popular webscraping tools   
                Beautiful Soup
                SCrapy
                Pandas
                Selenium
        data streams and feeds - senors . . . social media posts, time stamped with geo tags
            stock and market tickers
            retial transcation streams
            social media 
            secrutity 
            web clicks
            flight events
            Apps
                apache- kafka, spark storm

            RSS Really Simple Syndication
                Updates streamed to user devices

        Languages for data Pros:
            Python - good for beginners, preformers high-computational tasks
                Supports multiple programming paradigms
                    oop, imperative, functional, procedural
                    easier to learn, open source, windows/linux
                    Pandas
                    numpy/scipy
                    Beautiful soup and SCrapy
                    matpll
                    open sea    
                    
            R - developing statistical software, creating compelling visualizations
                open-source, platform-independent, can be paired with other programming languages
                highley extensible, can handle structured and Unstructured data
                ggplot2, plotly
                reports builit in
                create interactive webs
            JavaScript - cleaning data, import, export, stats, visualization
                hadiip hive spark
            Query, - database 
                insert, update, delete records, stored prosejuer
                advantages - portable, wide variety of databases, simple syntax, 
                    retrieve large amounts of data, runs on interpreter system 
            programming lang, - Apps
            shell scripting - timed operational tasks
                UNIX commands
                best used for repetitve tasks
                File manipulation
                program execution
                Sys admin stuff
                instrall scripts for complex programs
                execute routine backups
                running batches
                PowerShell -  ms    
                    optimized for working with structured data formats, automization
                        json csv, xml, api 
                        command line shell
                        object based - filter sort filter compare 
                        good for data mining, guis, charts, dashboards and interactive reports

Data Repositories: 
    General term to data that has been collected, orgaized, and isolated
    databases - DBMS - is a program, uses querying  
        -RDBMS optimized for data operations and querying, uses sql
        -Nosql - free form 
    Data Warehouses - merges info via ETL process - extract, transform and load into 1 database 
        historically relational, but can be non relational 
    Data STores - 
        distrubted computational 

    RDBMS:
        postgresql
        ACID compliance
        use cases, OLTP-Online Transaction Processing Application
            Data Warehouses OLAP
            IoT Solutions
        Limits:
            Does not work well with semi structured or Unstructured
            data values are limited per field
            migration between can be a challenge if things are set perfect
    
    NoSQL:
        Not only SQL or non-relational database
        flexible schemas 
        Built for specific data models 
        do not use fixed schemas and do not typically support sql
        Types:
            Key Value Store:
                key is attrubute- strings to json
                user session data
                real time
                inmemory 
                Rdeis memcached Dinamo db2
            Document based:
                store each record in a document 
                enable flexible indexing 
                preferred for eCommerce platforms, medical records storage, CRM platforms and analytics platforms 
                not good for complex search queries or perform multi-operational transactions 
                MongoDB, DocumentDB, CouchDB and Cloudant 
            Column-based:
                data stored in cells grouped as columns of data instead of rows
                logical grouping of columns is referred to as a column family
                great for systems that require heavy write requests, storing time-series data, weather data and IoT data 
                Not good for complex queries or changing querying patterns frequently 
                Cassandra, Apache HBase
            Graph Based:
                use a graphical model to represent and store data
                useful for visualizing, analyzing and finding connections between different pieces of data 
                excellent choice for working with data that has lots of interconnected relationships
                Good for social networks, product recommendations, network diagrams, fraud detection, access management
                Not good for processing high volumes of transactions 
                new4J, CosmosDB
        Advantages:
            ability to run as a distributed system scaled across multiple data centers 
            efficient and cost-effective scale-out architecture that provides additional capacity and performance with 
                the addition of new nodes
            simpler design, better control over availability, and impove scalability that makes it agile, flexible, and support quick itertions
        Not ACID compliant 

        Data Warehouse:
        Data Warehouses, that consolidate incoming data into one comprehensive storehouse.  
            multi purpose storage, already been ETL and ready for analytics, single source of truth
            operational and performance database 
            Data Mart: 
            Data Marts, that are essentially sub-sections of a data warehouse, built to isolate data for a particular business function or use case
                sub group of data warehouses
                built for specific group within company
                provide analytical capabilites for a restricted area of the data warehouses 
                islotated security and isolated performance
        Data Lake:
            Pool of raw data
                data element is given a unique identifier and is tagged with metatags for further use 
            data is selected and organized based on the use case you need it for
            retains all source data without exclusions
            staging area before data warehouse
            used for predictive and advanced analytics
            Data Lakes, that serve as storage repositories for large amounts of structured, semi-structured, and unstructured data in their native format. 
        ETL Process: 
            Extract, Transform and Load Process
                    Automated
                    converts to analysis ready
                    clean standardize
                    load data into repositories 
                Extract 
                    Batch Process - large chunks of data moved from source to destination at scheduled intervals 
                        stitch, blendo
                    Steam processing - data pulled in real-time from source, transfromed in transit and loaded into data repositories 
                        samza storm kafka
                Transform:
                    standardizing date formats and units of measurement
                    removing, filtering, enriching(splitting names)(establishing key relationships across tables))(apply business rules)
                Load:
                    -transportation of processed data in to a data repository
                        -intitial loading - populating all of the data in the repository
                        -incremental loading - applying updates and modifications periodically
                        -full refresh - erasing a data table and reloading fresh data
                    -load verification
                        -missing or null values
                        -server performance
                        -load failures

    Big Data Stores, that provide distributed computational and storage infrastructure to store, scale, and process very large data sets.
    
        Data Pipelines:
            Encompasses the entire journey of moving data from one system to another, including the ETL process
            can be used for both batch and streaming data
            Typically loads into a data lake
            Apache Beam, Data Flow, Kafka
        
    Big Data:
        Velocity 
        Volume 
        Variety 
        Veracity  quality, accuracy 80% of data is Unstructured
        Value need to make data valuable, medical social financial 

        NoSQL and Data Lakes 
        Apache hadoop - a collection of tools that provides distributed storage and processing of big data 
                -Java based, distributed storage and processing of large datasets across clusters of computers 
                -better real time data driven decisions
                -incorporates emerging data formats not traditionally used in data warehouses
                -provides real-time self-service access to stakeholders
                -optimizes and streamlines costs by consolidating data, inlcuding cold data, across the organization
            HDFS - storage system for big data that runs on multiple commodity hardware connected through the network   
                -provides scalable and reliable big data storage by partitioning files over multiple nodes
                -splits large files across multiple computers, allowing parallel acces to them
                -replicates file blocks on different nodes to prevent data loss
                    -multiple copies of data for higher availability
                    better scalability and data locality
                -fast recovery from hardware failures
                -better access to streaming data 
                -accommodation of large data sets 
                -accommodation of larger data sets due to nodes being scaled to hundreds
                -super compatible
            -open-source data warehouse software for reading, writing and managing large data set files that are stored directly in either
                HDFSor other data storage systems such as Apache HBase
        Apache hive - data warehouse for data query and analysis
            -Hive is based on Hadoop
            -Not suitable for 
            -transaction processing that involves a high percentage of write operations 
            -Better suited for warehousing tasks such as ETL, reporting and data analysis
        Apache Spark - distributed analytics framework for complex, real-time data analytics
            -general purpose data procesing engine designed to extract and process large volumes of data for a wide range   
                of Applications
                    -streams processing
                    -Machine learning
                    -interactive Analytics
                    -Data Integration
                    -ETL 
            -Has in memory processing which significatlly increases speed of computations
            -Provides interfaces for major programming languages such as Java, Scala, Python, R and SQL 
            -can run using its standalone clustering technology 
            -can also run on top of other infrastructures such as Hadoop 
            -can access data in large variety of data sources, including HDFS and Hive 
            -process streaming data fast
            -process complex analytics fast

**********************************
    Process for Identifying Data:
        -determine the info you want to collect 
            specific info you need
            possible sources for Data
        -define a plan for collecting data
            -establish timeframe for collecting data
            -how much data is sufficient for a credible analysis
            -define dependencies, risks, and mitigation plan
        -determine data collection methods
            -sources of data
            -type of data
            -timeframe over which you need the data 
            -volume of data
        
        data you identify, the source of that data and the practices you employ
        for gathering the data have implications for 
            quality
            security
            privacy

        In order to be reliable, data needs to be:
            Free of Errors, Accurate, Complete, Relevant, Accessible

        Data Governance policies and procedures relate to the usability, integrity and 
        availability of data.

        Data privacy issues include:
            Confidentiality, Compliance to mandated regs., License for use
                You need to define: **********
                    Checks, Validations, Auditable trail
    
    Data Sources:
        Internal and External data to an organization   
            Primary,- data from orgs. CRM, HR, or workflow apps 
                data you gather directly from surveys, interviews, discussions, Oberservations
                    and focus group
            Secondary - external databases, research articles, publications, training material, internet 
                searches or financial records available as public data, external surveys, interviews,
                external discussions or external Oberservations and focus groups
            Third-party - data purchased from aggrators purchased from other organizations

        databases- can be primary, Secondary or third party
        web- 
        social media - 
        sensor data - 
        data exchange - third party data source 
        surveys - 
        Census data - 
        Interview - for gathering qualitative data 
        Oberservation studies - could be all 3

        Try to supplement data with all of the different sources, Primary, Secondary and Third

    How to Gather and import data:
        SQL for relational databases
        SQL, CQL-Cassandra, GraphQL-Neo4J for non relational
        APIs - invoked from apps that access and endpoint containing data   
            endpoint can include databases, web services and data marketplaces
            -also used for data validation like postal address or zip codes
        Web Scraping - downloading specific dat from web pages
                        text, contact info, images, videos, podcast and product items from web property
            RSS feeds - for capturing
        Data Streams - Instruments, IoT devices, Apps, Car GPS, social media sites and interactive platforms
        Data Exchange Platforms - facilitate the exchange of data, 
            -provide data licensing workflows, de-identification and protection of personal info
                leagl frameworks, and a quarantined analytics enviroment
            -Ex. AWS DataExchange, Crunchbase, Lotame, Snowflake
        Other Sources - 
            Forrester and Business Insider for marketing trends and ad spending
            Gartner and Forrester for strategic and operational guidance
            Agencies for user behavior data, mobile and web usage, market surveys, and 
                demographic studies.

        Importing Data:
            Combine data from different sources into data repositories
                -needs to be optimized
            Structured data: OLTP systems, spreadsheets, online forms, sensors, network and web logs
                also nosql
            Semi-structured data: emails, XML, zipped files, binary executables and TCP/IP protocols
                -can be stored in NoSQL clusters
                -XML and JSON are commonly used for storing and exchanging semi-structured data 
            Unstructured data: web pages, social media feeds, images, videos, documents, media logs
                                        and surveys 
                -Can be stored in NoSQL databases and data lakes
                -ETL tools and data pipelines provide automated functions that facilitate   
                    the process of importing data   
                        Ex of programs - talend, informatica and Python, R

What is data Wrangling:
    Also known as Data Munging:
        Iterative process that involves data exploration, transformation, validation and marketing
        it available for credible and meaningful analysis.
    4 steps: 
        ***Discovery/exploration   
            -examining and understanding your data 
            -creating a plan for cleaning, structuring, organinzing and mapping data 
        ***Transformation 
            -bulk of process 
            -structuring data 
                -possibly combining data from nrdbms and api    
                    -with joins or unions potentially
                        -joins combine tables but keep columns separated
                        -unions combine single table(video says rows, 
                                        think vertical integration of tables)
            -normalizing and denormalizing
                -normalizing 
                    -cleaning unused data
                    -reducing redundancy
                    -reducing inconsistency
                -denormalizing
                    -combining data from multiple tables into a single table for faster querying
                    of data for reports and analysis
                        -transactions?
            -cleaning
                -fix irregularities in data in order to produce a credible and accurate analysis
                    - missing data, incomplete data, Biases in data, Null Values, Outliers 
                        -sale of certain product that did not capture the gender 
            -enriching 
                - adding more data points to make analysis more meaning full 
                    - gathering data from mulitple systems or public data sets  
                        - sell IT periferals
                            -look for public data sources   
                            - meta data 
        ***Validation 
            -checking the quality of data after above steps
            -verifying consistency, quality and security of data
        ***Publishing
            - delivering the output of the wrangled data for downstream project needs
          -Documentation
            - important to document steps took and your consideration for taking those steps, 
                to convert raw data into analysis-ready data
            - Document everything
    
    Tools for Data Wrangling: 
        -Excel Power Query/spreadsheets 
            - can import using Query
        -OpenRefine
            - can import or export data in TSV, CSV, XLS, XML and JSON
            - open source
            - can clean, transform/convert format, extend data with web services and external data 
            - easy to learn and use
            - offer menus
        -Google DataPrep
            - intelligent cloud data service
            - visually explore, clean and prepare both structured and unstructured data 
            - fully managed service 
            - data prep super east
            - offers suggestions on ideal next steps
            - automatically detects schemas, data types and anomalies
        -Watson Studio Refinery
            - need IBM Watson Studio
            - discover, cleanse and transform data with built in operations
            - transforms large amount of raw data  into consummable, quality information ready for analytics
            - offers flexibility of exploring data residing in a spectrum of data sources
            - detects data types and classifications automatically
            - enforces applicable data Governance policies automatically 
        -Trifacta Wrangler
            - interactive cloud based service for cleaning and transforming data
            - takes messy, real-world data and cleans and rearranges it into data tables
            - can export to excel, tableau and R 
            - known for its collaboration features ******** 
        -Python
            - huge library and packages
                - Jupyter Notebook
                - NumPy 
                -Pandas -data anlalysis operations  
                    - merging, joining, transforming huge chunks of data using simple, single line commands 
                    - helps prevent common errors that result from misaligned data coming in from different sources       
        -R 
            - also has libraries and packages explicitly for wrangling messy data
                - investigate, manipulate and analyze data
                -Dplyr - powerful library for data wrangling with a precise straightforward syntax
                -Data.table - helps appregate large data sets quickly
                -Jsonlite - robust JSON parsing tool, great for interacting with web APIs 

        Make the right data tool choice:
            -case, infrastructure and team
                -supported data size
                -data structure
                -cleaning and transformation capabilites
                -infrastructure needs
                -ease of use
                -learnability
    
    Data Cleaning: 
        Part of the above transformation process

        Data Cleaning Workflow:
            Inspection
                -detecting issues and errors
                -validating against rules and constraints
                -profiling data to inspect source data
                -visualizing data using statistical methods 
            Data Profiling
                -sturcture 
                -content
                -Inter-relationships
                    - this uncovers anomalies and data quality issues
                -visualizing
            Cleaning
                - will depend on use case and type of issues
                - missing values 
                    - filter out or source missing info
                    - imputation, calculated based on statistical values
                - duplicate data
                - irrelevant data   
                    - not contextual to your case
                - data type conversion
                - standardizing data 
                - syntax errors
                    - white spaces, extra spaces, typos and formatting issues
                - outliers  
                    - examined for accurcay and inclusion
            Verification
                - inspecting results to establish effectivness and accuracy achieved as a result 
                    of the data cleaning
                - Documentation 
                    - changes undertaken as part of the data cleaning operational
                    - reasons for undertaking these changes
                    - quality of currently stored data  

            Data Preparation and Reliability Viewpoints:
                -Run summary statistics on data to make sure it is consistent with reality
                    -mean, median, max
                - reliable, non biased and free from errors
                - perform a logic check at high level 
                - ensure basic data integrity question are addressed before analysis begins 

Overview of Statistical Analysis:
    What is statistics? 
        - a branch of mathematics dealing with the collection, analysis, interpretation and 
            presentation of numerical or quantitative data 
            ex. averages, vaccine data, reducing customer churn
    Statistical Analysis    
        - the application of statistical methods to a sample of data
            in order to develop an understanding of what that data
            represents
            - sample - a representative selection drawn from a total population
                - population - a discrete group of people or things that can be identified by at least 
                    one common characteristic for puposes of data collection and analysis 
        
        - statistical methods help ensure   
            - data is interpreted correctly
            - apparent relationships are meaningful 

        - 2 Types of statistics:
            - Descriptive - summarizing info about the sample
                - present data in a meaningful way
                - simpler interpretation of data, summary charts, data and graphs
                - not a hypothesis

                - central tendency - locating center of data sample 
                    - mean - average
                    - median - middle value
                    - mode - most common value in data set 
                - dispersion - measure of variability in a datasets
                    - Variance - how far data point fall from center 
                        -lower variability - consistent values in a dataset 
                        - higher variability - data points are dissimilar, likely hood of extreme values
                    - Standard Deviation - how tightly your data is clustered around the mean 
                    - Range - distance between the smallest and largest values in your data 
                - skewness - measure of whether the distribution of values is symmetrical around 
                                a central value or skewed left or right 
                            - can affect which types of analyses are valid to perform 
                            -correlation and scatter plots  
            - Inferential - making inferences or generalizations about the broader population 
                    -you can draw generalizations that apply the results of the 
                        sample population as a whole 
                
                - Hypothesis Testing - studying the effectiveness of a vaccine by comparing outcomes    
                                        in a control group, 
                                -can tell you whether the efficacy of a vaccine obserbed in a control group
                                            is likely to exist in the population as well 
                - Confidence Intervals - incorporte the uncertainty and sample error to create a range of values
                                             the actual population value is likely to fall within 
                - Regression Analysis - incoporates hypothesis tests that help determine whether the 
                                        relationships observed in the sample data actually exist in the population
                                        rather than just the sample

                Conclusion:
                    Stats combined with data mining 
                        - provides measures and methodologies necessary for data mining 
                        - Identifying patterns that help identify diffs between random noise and signficant fingings 
            Tools for Statistics:
                SAS, SPSS, StatSoft

    What is Data Mining?:
            - the process of extracting knowledge from data
            - interdisciplinary field that involves the use of pattern recognition technologies, statistical analysis
                and mathmatical techniques
            - aims to identify correlations in data, find patterns and variations, understand trends
                and predict probabilities

        Patterns and Trends:
            - pattern recognition is the discovery of regularities or commonalities in data
                - user log data, 
                    - analysis of user behavior
            - trend is general tendency of a set of data to change over time    
                - global warming?!?!?

            Ex. Profiling customers behaviors, needs and disposable income in order to offer tageted campaigns
             - tracking customer transactions for unusual behaviors and flagging fraudulent transactions using data mining models 
             - using statistical models to predict a patient's likelihood for specific health conditions and prioritizing treatment 
             - accessing performance data of students to predict achievement levels and make a focused effort to provide support where required 
             - helping investigation agencies deploy plice force where the likelhood of crime is higher 
             -aligning supply and logistics with demand forecasts 
        
        Data Mining Techniques:
            - Descriptive 
            - Diagnostic
            - Predictive 
            - Prescriptive Modeling 

            - Classification 
                - classifying attributes into target categories 
            - Clustering    
                - involves grouping data into clusters so they can be treated as groups 
                - based on geographic regions 
            - Anomaly or Outlier Detection 
                - finding patterns in data that are not normal or unexpected 
                - spikes in usage of credit Cassandra
            - Association Rule Mining   
                - establishing a relationship between two data events 
                - purchase of laptop with cooling pad 
            - Sequential Patterns 
                - tracing a series of events that take place in a sequence
                - customers shopping trail on online store
            - Affinity Grouping 
                - discovering co-occurence in relationships 
                - recommending products based on other peoples buying habits that are similar 
            - Decision Tree
                - building classification models in the form of a tree structure with multiple branches, 
                    where each branch represents a probable occurence 
                - helps to define a clear understaning of input and output 
            - Regression 
                - identifying the nature of the relationship between two variables, which could be causal or correlation 
                - based on factors of location and area to predict value of house 
            
            Conclusion:
                - Helps businesses focus on what is relevant 
    
    Tools for Data Mining:
        - Spreadsheets 
            - basic data mining tasks 
            - hosting data that has been exported from other systems in easily accessible and easy to read format 
            - pivot tables to showcase specific aspects
            -easier to compare sets 
            -Excel addins - allow you to perform common mining tasks such as classification, regresson, association rules, clustering and model building 
                -Data Mining Client
                -XLMiner
                -KnowledgeMiner
            -Google Sheets add ons- 
                -Text Analysis 
                -Text Mining 
                -Google Analytics 
        - R - Most widely used languages for performing statistical modeling and computations by statisticians and data miners 
            - libraries operations - regression, classification, data clustering, association mining, text mining, outlier detection, social network anlysis 
            -tm - framework for text mining apps within R 
            -twitteR - framework for mining tweets 
            -RStudio popular open-source Integrated Development enviroment or IDE for working with R 
        - Python 
            - Pandas -  data structures and analysis, can upload data in any format and provides simple platform to organize, sort and manipulate data 
                - basic numerical computations mean, median, mode and range
                - calculate statistics and answer questions regarding correlation between data and distribution of data 
                - explore data visually and quantitatively 
            - Numpy - tool for mathematical computing and data preparation in Python
                - offers built in functions and capabilites for data mining 
            - JupyterLab - tool of choice for Data Scientist and Data Analysts when working with Python to perform data 
                        mining and statistical analysis 
        - IBM SPSS Statistics 
            - Statistical Process for Social Sciences 
                - popularly used for advanced analytics, text analytics, trend analysis, validation fof assumptions, and 
                    translation of business problems into data science solutions 
                - closed source
                - needs License
                - minimal coding for complex tasks
                - efficient data management tools 
                - popular for its in-depth analysis capabilites and accurate data results 
        - IBM Watson Studio 
            - included in IBM Cloud Pak for data 
                - leverages a collection of open source tools such as Jupyter notebooks
                    and extends them with closed source IBM tools that make it a powerful   
                    enviroment for data analysis and data science 
            - available through a web browser on the public cloud, private cloud and as a descktop app 
            - enables collaboration, 
            - includes SPSS Modeller flows that enable you to quickly develop predictive models for your business data 
        - SAS Enterprise Miner
            - provides powerful capabilities for interactive data exploration 
            - can manage information from varous sources, mine and transform data and analyze statistics 
            - offers graphical user interface 
            - identify patterns in the data using a range of available modeling techniques
            - explore relationships and anomalies in data
            - analyze big data 
            - validate the reliability of finding from the data analysis process 
            - easy to use syntax 
            - high security 





Overview of Communicating and Sharing Data Analysis Findings: 
    Data Projects - 
        - collaborative efforts across business functions
        - people with multi-disciplinary skills
        - finding incorporated into larger business iniciative 
        - Success of your communication depends on how well others can understand and trust your insights to take further action 
            -story 
            -visualization
            -data 
        Questions?
            -Who is my audience? 
            -What is important to them?
                -what and how much info is essential 
                -don't bring out all the data, presentation is not a data dump 
            -What will help them trust me?
                -begin presentation by demonstrating your understanding of the business problem 
                -speak in language of organizations domain 
                -structure presentation for maximum impact 
                    -reference your data 
                    -state your assumptions
                    -organize representation 
                    -identify best formats for presenting your data 
                    -Role of Visualizations 
                        -graphs, charts, diagrams 
                -audience needs to trust, understand and relatability
                    -establish credibility
                    -present data with a narrative
                    -support the narrative with visual impressions 
    
    Storytelling with Data Analysis:
        - Storytelling is critical 
            -be clear, concise and compelling 
        - develop story while your researching and testing 
        - find balance between clarity and conveying complexities 
        - find best way to communicate 
    
    Intro to Data Visualization:
        - discipline of communicating information through the use of visual elements such as graphs, charts and maps. 
            - Its goal is to make information easy to comprehend, interpret and retain
        - Choosing Appropriate Visualizations 
            - What is the relationship that I am trying to establish?
            - Do I want to compare mulitple values, such as the number of products sold and revenues generated over the last 3 years?
            - Do I need my audience to see the correlation between 2 variables? 
            - Do I want to detect anomalies in data? 

            - What is the question I'm trying to answer? 
                - answer this question for audience with every dataset and information that you visualize 
                - consider whether the vis. is static or interactive
                - takeaway for my audience?
                - what does audience need to know?
                - what questions do they have? 

        - Common types of Graphs:
            -Bar charts
            -Column Charts - effective to show change over time 
            -Pie Charts - show break down of entity 
            -Line Charts - showing data value is changing in relation to a continuous variable 

        -Dashboards:
            - organize and display visualizations coming from multiple data sources into a single graphical interface 
            - can present both operational and analytical data 
                - birds eye view of the complete picture while also allowinng you to drill down into tthe next level of info for each parameter 
                - easy to comprehend by an average user 
                - make collab easier
                - generate reports on the go 
                - almost instant 
    
    Introduction to Visualization and Dashboarding Software:
        - Spreadsheets
            -most commonly used for graphical representation of data sets
            -easy to learn
            -tons of Documentation
            -Excel
                -bar charts, line charts, pie charts, pivot charts, scatter charts, trendlines, Gantt charts, Waterfall charts and Combination Charts 
                - provides recommendations on visual representation 
                -can add chart title, chang colors of elements, and add labels to data 
            -Google Sheets 
                -wide range of charts 
                -high light data and click chart button for vis. recommendations
                -preferred for mulitple users 
        - Jupyter Notebooks and Python libraries
            -open-source 
            -explor and create visualizations 
            -matplotlib
                -widely used python data visualization Library
                -probides 2d and 3d plots and the flexibility to create plots in several different ways 
                - high-quality interacitve graphs and plots with just a few lines of code 
                - large community support and cross-platform support 
            -bokey 
                - interacitve charts and plots 
                - high performance interactivity over large or streaming datasets
                - flexibility for applying interation, layouts and different styling options to visualization 
                - can transform some visualizations written in other Python Libraries, such as matplotlib, seaborn and Ggplot 
            -Dash 
                - python framework for creating interactive web-based visualizations
                - can build highly interacitve web apps using Python 
                - does not require knowledge of HTML and javascript 
                - easily maintainable, cross-platform and mobile-ready 
        - R-Studio
            - basic visualizations such as histograms, bar charts, line charts, box plots and scatter plots 
            - advanced visualizations heat maps, mosaic maps, 3d gaphs, and correlograms 
            - R-Shiny
                - R package 
                - can build interactive web apps that can be hosted as stand alone apps on a webpage 
                - build dashboards, ease helped make it popular 
        - IBM Cognos Analytics 
            - end-to-end analytics solution 
            - 
        - Tableau 
        - Microsoft Power BI 

            

