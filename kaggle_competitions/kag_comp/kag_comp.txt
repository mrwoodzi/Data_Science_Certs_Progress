

Notes and planning for this project which is my first kaggle competition: 
    requirements:
        -basic analysis libraries
            jupyterlab
            python
            numpy
            pandas
            matplotlib
            seaborn
        
        -audio processing libraries 
            -librosa

        -computation libraries  
            -scikit learn
            -snapml
            -joblib

Links to research papers that were helpful: 
    https://www.researchgate.net/publication/376366076_Vibe_-_With_people_with_hearing_impairment
    https://www.researchgate.net/publication/271608436_Design_and_Implementation_of_an_Audio_Classification_System_Based_on_SVM



    How do I do this?

        MFCC (Mel-Frequency Cepstral Coefficients):
            -MFCCs are a popular feature extraction method for speech and audio processing. They 
                involve taking the Mel spectrogram, applying a logarithm, and then performing a 
                discrete cosine transform (DCT) to obtain a compact representation of the spectrum.
            -robust to noise and environmental variations.

        Chroma Features:
            -Chroma features capture the harmonic content and distribution of energy in different pitch classes (i.e., 
                musical notes) over time. They are particularly useful for tasks involving music 
                analysis, such as chord recognition and music genre classification.
            -They are particularly useful for distinguishing between different bird species 
                based on their vocalizations.

        Spectral Contrast:
            -Spectral contrast measures the difference in energy between peaks and valleys in the 
                spectrum. It provides information about the spectral texture of audio signals and 
                is often used in music genre classification and audio segmentation.

        Zero-Crossing Rate (ZCR):
            -ZCR measures the rate at which the audio signal changes sign (crosses the zero amplitude axis) 
                over time. It can be used to estimate properties such as pitch and timbre and is commonly 
                used in speech and music processing tasks.

        RMS Energy:
            -Root Mean Square (RMS) energy measures the average energy of the audio signal over time. It 
                provides information about the overall amplitude or loudness of the signal and can be used 
                for tasks such as audio event detection and speech recognition.

        Pitch and Spectral Centroid:
            -Pitch represents the perceived frequency of a sound, while spectral centroid measures the center 
                of mass of the frequency spectrum. These features are useful for tasks involving pitch estimation 
                and sound source localization.

        Time-Domain Features:
            -Time-domain features include statistical measures such as mean, variance, skewness, and kurtosis of 
                the audio signal. These features provide information about the temporal characteristics of the signal 
                and can be used for various audio analysis tasks.


        Potential Path: 
            Feature Extraction: 
                -Extract MFCCs, Chroma features, and spectral contrast features from the bird call recordings using a library like librosa.
                -time domain & pitch/spectral centroid extraction  - https://www.researchgate.net/publication/271608436_Design_and_Implementation_of_an_Audio_Classification_System_Based_on_SVM
                -mfcc, high zero-crossing rate ratio, low short-time energy ratio, frequency energy
            Feature Preprocessing:
                Normalize the extracted features to ensure that they have zero mean and unit variance.
            Feature Vector Construction:
                Concatenate the MFCCs, Chroma features, and spectral contrast features to form a single feature vector for each bird call sample.
            Labeling:
                Assign labels to the bird call samples based on their corresponding bird species.
            Training the SVM Classifier:
                Split the dataset into training and testing sets.
            Train an SVM classifier using the combined features as input features and the corresponding labels as target labels.
                Evaluation:
            Evaluate the performance of the trained SVM classifier using the testing data.
                Calculate metrics such as accuracy, precision, recall, and F1-score to assess the classifier's performance in classifying bird calls.

            -SVM notes:
                -parameter tuning is very important with svm 
                -maybe,  "For multi-class, the most popular choice is one-vs-all classification"

        
        General Notes on Audio Classification:
            The  audio  classification  and  recognition  research  consists  of  three  fundamental  disciplines: 
                sound event recognition, popularly known as ESC [15], automatic recognition of  speech  [16],  
                and music category classification [17]. https://www.researchgate.net/publication/345260947_Efficient_Classification_of_Environmental_Sounds_through_Multiple_Features_Aggregation_and_Data_Enhancement_Techniques_for_Spectrogram_Images

            Tonal Centroid is a depiction of a pitch. It is also recognized as a harmonic network
            Chroma feature includes the division of the audio signal pitch into two components  in  [63].  One  is  chroma,  and the  other is  
                tone height.facilitate
            The spectral components have been achieved by performing the fast fourier transform (FFT) on each frame. In the next step, 
                these components are segregated into six octave-scale-based sub-bands, one by one. At last, the strength of  spectral 
                valleys and peaks has been  estimated  by the small neighborhood, average value. The detailed expressions have been demonstrated as follows:

                Another important detail that we realized from Table 5 is that the best accuracy achieved by  those  featuresâ€™ aggregation techniques  were  obtained 
                from  the  Mel filter bank  approach.  The training time of the triple accumulated features is also less than the double conglomerated features.
                Our approach is the combination of  two  methodologies  transfer  learning  with  fine-tuning  and  features  aggregation-based  data enhancement techniques.






    Notes on Neural Networks: 
        -Convolutional Neural Networks  
            -particularly effective for image recognition and processing tasks  
                -inspired by the organization of the animal visual cortex, where individual neuron  respond
                    to stimuli only in a restricted region of the visual field known as the receptive field
                -designed to automatically and adatively  learn spatial hierarchies of features from input images 
            -optimized using Adam Optimizer and the model trained using a cross-entropy loss function
                -deployed on mobile device 


    
    Research Paper Notes: 
        UbiEar
        https://www.researchgate.net/publication/376366076_Vibe_-_With_people_with_hearing_impairment
            -uses lightweight Deep Convolutional Neural Network 
                -average accuracy 91.2%
            -through the integration of EdgeML techniques we aim to create a solution that not only conserves
                resources but also significantly lowers cost. Our priority lies in achieving the lwest pssible latench 
                and power consumption...

            Neural Network Architecture: 
                1. Input Layer (6,435 features):
                    -first layer of the neural network; this layer will take the preprocessed audio samples as input 
                2. Reshape layer: 
                    -input audio signals are first transformed into a spectrogram. The resulting spectrogram is a 2d 
                        matrix of frequency bins and time frames. However, the input to the neural network must be in the form 
                        of a 3D tensor, where the third dimension represents the number of input channels 
                3. 1d convolutional/pool layer (8neurons, three kernel size, one layer):
                    -would take as input a 1D tensor(i.e., vector) and apply eight filters, each with a size of 3, to 
                        produce eight feature maps. The ouput would be a 3D tensor with dimensions (batch_size, sequence_lenght - 2, 8),
                        where batch_size is the number of samples in a batch and sequence_length is the length of the input sequence. 
                4. Dropout Layer: 
                    -a rate of 0.25 is added after the first dense layer. During each training iteration, 25% of the neurons 
                        in the first dense layer will be randomly dropped out. 
                5. 1D convolutional/pool layer (16neurons, three kernel size, one layer):
                    -would take a 1d tensor, and apply 16 neurons, each with a size of 3 to produce 16 feature maps. The output would be 
                        a 3D tensor with dimensions(batch_size, sequence_length - 2, 16), where batch_size is the number of samples in a batch 
                        and sequence_length is the length of the input sequence. Adding a 1D pooling layer after the convolutional layer would 
                        further reduce the dimensionality of the output feature maps. A typical choice for the pooling operation in a 1D CNN is 
                        to use the maximum value in each sliding window of pool.
                6. Output Layer: 
                    -responsible for producing the final output of the network based on the input data, final output of the network 
                        based on the input data, and uses a softmax activateion function to produce a probability distribution over the 
                        different classes in the classification problem. the class with the highest probability is then selected 
                        as the predicted output of the network. 

            Methodology: 
                -data collection
                -Preprocessing
                    -subsampling
                        -subsampled data to 16khz
                            -reduces the computational complexity of training a neural network and the 
                                memory requirements of the training and deployment process, helps to prevent 
                                overfitting(model becomes too specific on training data)
                                    -p670 [27,28]
                    -de-noising
                        -removing distracting sounds
                            -can be done using spectral subtraction, Weiner filtering and adaptive filtering
                            -goal is to improve Signal-To_Noise Ratio(SNR)
                            -used Scipy to design bandpass filters that remove the noise identified frequency range for each sound class
                            --Definitely want to do this, probably high pass filter since it's bird calls
                    -silence portion detection and removal
                        -removing silent portions from the recorded sound files before training a neural network also helps improve the 
                            accuracy of the model prediction 
                            -silent portions often  caused by background noise, 
                                -interference or gaps between sound events. 
                -feature extraction
                    -Spectrogram 
                        -spectrogram parameters are the settings that can be adjusted to control the appearance
                            and quality of the spectrogram  
                                -window size: 1000ms
                                -FTT size: 128
                                -Frequency range: 0-8khz
                                -Noise Floor: (-52db) - minimum amplitude threshold for audio samples in the dataset    
                                    -not sure if noise at -52db will be good for bird calls
                        -compute Short-Time Fourier Transform of the audio signal   
                            -divided the signal into overlapping frames and applying a Fourier transform to each frame
                            -variation in freq content of the audio signal over time is  then presented as a 2-D figure
                        -the above extracted features from the sample can then be used as inputs to the next stage, i.e., 
                            training a Convolutional Neural Network 
                -labeling 
                -model training 
                -evaluation 
                -deployment to edge devices  


Random Rabbit Holes: 
    -TensorFlow LSTM